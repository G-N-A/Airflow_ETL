[2025-09-08T13:20:03.003+0530] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-09-08T13:20:03.015+0530] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:40:00+00:00 [queued]>
[2025-09-08T13:20:03.018+0530] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:40:00+00:00 [queued]>
[2025-09-08T13:20:03.018+0530] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2025-09-08T13:20:03.074+0530] {taskinstance.py:2330} INFO - Executing <Task(BashOperator): run_etl_pipeline> on 2025-09-08 07:40:00+00:00
[2025-09-08T13:20:03.082+0530] {standard_task_runner.py:64} INFO - Started process 712606 to run task
[2025-09-08T13:20:03.088+0530] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:40:00+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/ecom_schedules.py', '--cfg-path', '/tmp/tmpna0xkvhn']
[2025-09-08T13:20:03.089+0530] {standard_task_runner.py:91} INFO - Job 24: Subtask run_etl_pipeline
[2025-09-08T13:20:03.170+0530] {task_command.py:426} INFO - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:40:00+00:00 [running]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME
[2025-09-08T13:20:03.330+0530] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecom_etl_10min' AIRFLOW_CTX_TASK_ID='run_etl_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2025-09-08T07:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-09-08T07:40:00+00:00'
[2025-09-08T13:20:03.332+0530] {taskinstance.py:430} INFO - ::endgroup::
[2025-09-08T13:20:03.332+0530] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-09-08T13:20:03.333+0530] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/scripts/etl/run_etl.sh | cat']
[2025-09-08T13:20:03.341+0530] {subprocess.py:86} INFO - Output:
[2025-09-08T13:20:03.352+0530] {subprocess.py:93} INFO - [2025-09-08 13:20:03] ðŸš€ Starting E-Commerce ETL Pipeline...
[2025-09-08T13:20:03.352+0530] {subprocess.py:93} INFO - ----------------------------------------
[2025-09-08T13:20:03.353+0530] {subprocess.py:93} INFO -  Source DB   : ecom_db
[2025-09-08T13:20:03.353+0530] {subprocess.py:93} INFO -  Target DB   : ecom_dwh
[2025-09-08T13:20:03.354+0530] {subprocess.py:93} INFO -  Server      : localhost:1433
[2025-09-08T13:20:03.354+0530] {subprocess.py:93} INFO -  Spark Master: local[*]
[2025-09-08T13:20:03.354+0530] {subprocess.py:93} INFO - ----------------------------------------
[2025-09-08T13:20:03.354+0530] {subprocess.py:93} INFO - 
[2025-09-08T13:20:03.355+0530] {subprocess.py:93} INFO - [2025-09-08 13:20:03] ðŸ”„ Running ETL Pipeline with spark-submit...
[2025-09-08T13:20:04.253+0530] {subprocess.py:93} INFO - 25/09/08 13:20:04 WARN Utils: Your hostname, softsuave-ASUS-EXPERTCENTER-D700ME-D500ME resolves to a loopback address: 127.0.1.1; using 192.168.6.3 instead (on interface eno2)
[2025-09-08T13:20:04.255+0530] {subprocess.py:93} INFO - 25/09/08 13:20:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2025-09-08T13:20:07.541+0530] {subprocess.py:93} INFO - 25/09/08 13:20:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-09-08T13:20:08.002+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkContext: Running Spark version 3.4.2
[2025-09-08T13:20:08.019+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO ResourceUtils: ==============================================================
[2025-09-08T13:20:08.020+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-09-08T13:20:08.020+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO ResourceUtils: ==============================================================
[2025-09-08T13:20:08.020+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkContext: Submitted application: ECommerce_ETL_Pipeline
[2025-09-08T13:20:08.035+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-09-08T13:20:08.041+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO ResourceProfile: Limiting resource is cpu
[2025-09-08T13:20:08.041+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-09-08T13:20:08.075+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SecurityManager: Changing view acls to: softsuave
[2025-09-08T13:20:08.075+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SecurityManager: Changing modify acls to: softsuave
[2025-09-08T13:20:08.075+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SecurityManager: Changing view acls groups to:
[2025-09-08T13:20:08.075+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SecurityManager: Changing modify acls groups to:
[2025-09-08T13:20:08.075+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: softsuave; groups with view permissions: EMPTY; users with modify permissions: softsuave; groups with modify permissions: EMPTY
[2025-09-08T13:20:08.210+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Utils: Successfully started service 'sparkDriver' on port 37419.
[2025-09-08T13:20:08.249+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkEnv: Registering MapOutputTracker
[2025-09-08T13:20:08.272+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkEnv: Registering BlockManagerMaster
[2025-09-08T13:20:08.282+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-09-08T13:20:08.282+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-09-08T13:20:08.285+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-09-08T13:20:08.299+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5ad868be-179e-44fa-ad91-6e23b87562d1
[2025-09-08T13:20:08.311+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2025-09-08T13:20:08.320+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-09-08T13:20:08.402+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-09-08T13:20:08.440+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-09-08T13:20:08.462+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO SparkContext: Added JAR file:///media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/libs/jars/mssql-jdbc-12.2.0.jre8.jar at spark://192.168.6.3:37419/jars/mssql-jdbc-12.2.0.jre8.jar with timestamp 1757317807997
[2025-09-08T13:20:08.502+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Executor: Starting executor ID driver on host 192.168.6.3
[2025-09-08T13:20:08.506+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-09-08T13:20:08.515+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Executor: Fetching spark://192.168.6.3:37419/jars/mssql-jdbc-12.2.0.jre8.jar with timestamp 1757317807997
[2025-09-08T13:20:08.547+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO TransportClientFactory: Successfully created connection to /192.168.6.3:37419 after 18 ms (0 ms spent in bootstraps)
[2025-09-08T13:20:08.557+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Utils: Fetching spark://192.168.6.3:37419/jars/mssql-jdbc-12.2.0.jre8.jar to /tmp/spark-b42c6cf1-ecca-4453-8b14-62fe77dbe9f8/userFiles-6a38e981-a216-40ef-a090-05e737ce7c59/fetchFileTemp7792249424037334476.tmp
[2025-09-08T13:20:08.576+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Executor: Adding file:/tmp/spark-b42c6cf1-ecca-4453-8b14-62fe77dbe9f8/userFiles-6a38e981-a216-40ef-a090-05e737ce7c59/mssql-jdbc-12.2.0.jre8.jar to class loader
[2025-09-08T13:20:08.580+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37121.
[2025-09-08T13:20:08.580+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO NettyBlockTransferService: Server created on 192.168.6.3:37121
[2025-09-08T13:20:08.581+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-09-08T13:20:08.584+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.6.3, 37121, None)
[2025-09-08T13:20:08.586+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.6.3:37121 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.6.3, 37121, None)
[2025-09-08T13:20:08.588+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.6.3, 37121, None)
[2025-09-08T13:20:08.588+0530] {subprocess.py:93} INFO - 25/09/08 13:20:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.6.3, 37121, None)
[2025-09-08T13:20:08.783+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:08,783 - __main__ - INFO - Starting ETL pipeline - Batch ID: 20250908075008
[2025-09-08T13:20:08.783+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:08,783 - __main__ - INFO - Extracting data from table: customers
[2025-09-08T13:20:11.731+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:11,731 - __main__ - INFO - Extracted 1000 records from customers
[2025-09-08T13:20:11.731+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:11,731 - __main__ - INFO - Extracting data from table: products
[2025-09-08T13:20:11.834+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:11,834 - __main__ - INFO - Extracted 500 records from products
[2025-09-08T13:20:11.834+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:11,834 - __main__ - INFO - Extracting data from table: orders
[2025-09-08T13:20:11.973+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:11,973 - __main__ - INFO - Extracted 10000 records from orders
[2025-09-08T13:20:11.974+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:11,973 - __main__ - INFO - Extracting data from table: order_items
[2025-09-08T13:20:12.057+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,057 - __main__ - INFO - Extracted 17319 records from order_items
[2025-09-08T13:20:12.058+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,057 - __main__ - INFO - Extracting data from table: inventory
[2025-09-08T13:20:12.129+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,129 - __main__ - INFO - Extracted 500 records from inventory
[2025-09-08T13:20:12.129+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,129 - __main__ - INFO - Extracting data from table: clickstream
[2025-09-08T13:20:12.210+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,210 - __main__ - INFO - Extracted 95799 records from clickstream
[2025-09-08T13:20:12.210+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,210 - __main__ - INFO - Cleaning customer data...
[2025-09-08T13:20:12.779+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,779 - __main__ - INFO - Customer cleaning complete. Records: 1000 -> 1000
[2025-09-08T13:20:12.779+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:12,779 - __main__ - INFO - Cleaning product data...
[2025-09-08T13:20:13.158+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:13,158 - __main__ - INFO - Product cleaning complete. Records: 500 -> 500
[2025-09-08T13:20:13.158+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:13,158 - __main__ - INFO - Cleaning order data...
[2025-09-08T13:20:13.506+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:13,506 - __main__ - INFO - Order cleaning complete. Records: 10000 -> 10000
[2025-09-08T13:20:13.759+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:13,759 - __main__ - INFO - Cleaning order items data...
[2025-09-08T13:20:13.965+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:13,965 - __main__ - INFO - Order items cleaning complete. Records: 17319 -> 17319
[2025-09-08T13:20:14.183+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:14,183 - __main__ - INFO - Cleaning inventory data...
[2025-09-08T13:20:14.374+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:14,374 - __main__ - INFO - Inventory cleaning complete. Records: 500 -> 500
[2025-09-08T13:20:14.520+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:14,520 - __main__ - INFO - Cleaning clickstream data...
[2025-09-08T13:20:15.641+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:15,640 - __main__ - INFO - Clickstream cleaning complete. Records: 95799 -> 95799
[2025-09-08T13:20:16.127+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:16,127 - __main__ - INFO - Creating Customer 360 view...
[2025-09-08T13:20:16.410+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:16,410 - __main__ - INFO - Customer 360 view created with 1000 records
[2025-09-08T13:20:16.410+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:16,410 - __main__ - INFO - Creating Product Analytics view...
[2025-09-08T13:20:16.667+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:16,667 - __main__ - INFO - Product Analytics view created with 500 records
[2025-09-08T13:20:16.667+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:16,667 - __main__ - INFO - Generating data quality report...
[2025-09-08T13:20:16.899+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:16,898 - __main__ - INFO - Loading 1000 records to dim_customers in overwrite mode...
[2025-09-08T13:20:46.470+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:46,470 - __main__ - ERROR - Failed to load data to dim_customers: An error occurred while calling o926.jdbc.
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - : com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database "ecom_dwh" requested by the login. The login failed. ClientConnectionId:b9c77fee-797c-4bc2-b174-9732ffc3971e
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:305)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:138)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:42)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:6490)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:5068)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$100(SQLServerConnection.java:93)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:5002)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7685)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4048)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3487)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3077)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2919)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1787)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1229)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:123)
[2025-09-08T13:20:46.471+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:119)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-09-08T13:20:46.472+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:753)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:46,471 - __main__ - ERROR - ETL pipeline failed: An error occurred while calling o926.jdbc.
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - : com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database "ecom_dwh" requested by the login. The login failed. ClientConnectionId:b9c77fee-797c-4bc2-b174-9732ffc3971e
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:305)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:138)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:42)
[2025-09-08T13:20:46.473+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:6490)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:5068)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$100(SQLServerConnection.java:93)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:5002)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7685)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4048)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3487)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3077)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2919)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1787)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1229)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:123)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:119)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2025-09-08T13:20:46.474+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:753)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-09-08T13:20:46.475+0530] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:46,471 - __main__ - ERROR - ETL pipeline failed: An error occurred while calling o926.jdbc.
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - : com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database "ecom_dwh" requested by the login. The login failed. ClientConnectionId:b9c77fee-797c-4bc2-b174-9732ffc3971e
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:305)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:138)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:42)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:6490)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:5068)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$100(SQLServerConnection.java:93)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:5002)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7685)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4048)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3487)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3077)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2919)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1787)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1229)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
[2025-09-08T13:20:46.476+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:123)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:119)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-09-08T13:20:46.477+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:753)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-09-08T13:20:46.478+0530] {subprocess.py:93} INFO - 
[2025-09-08T13:20:46.751+0530] {subprocess.py:93} INFO - 2025-09-08 13:20:46,751 - py4j.clientserver - INFO - Closing down clientserver connection
[2025-09-08T13:20:46.844+0530] {subprocess.py:97} INFO - Command exited with return code 0
[2025-09-08T13:20:46.844+0530] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-09-08T13:20:46.925+0530] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T07:40:00+00:00, execution_date=20250908T074000, start_date=20250908T075003, end_date=20250908T075046
[2025-09-08T13:20:47.124+0530] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2025-09-08T13:20:47.139+0530] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-09-08T13:20:47.140+0530] {local_task_job_runner.py:222} INFO - ::endgroup::
