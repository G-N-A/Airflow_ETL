/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/utils/db.py:921 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-09-08T12:56:22.433+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-09-08T12:56:22.434+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2025-09-08 12:56:22 +0530] [705351] [INFO] Starting gunicorn 22.0.0
[2025-09-08 12:56:22 +0530] [705351] [INFO] Listening at: http://[::]:8793 (705351)
[2025-09-08 12:56:22 +0530] [705351] [INFO] Using worker: sync
[2025-09-08 12:56:22 +0530] [705352] [INFO] Booting worker with pid: 705352
[2025-09-08 12:56:22 +0530] [705353] [INFO] Booting worker with pid: 705353
[[34m2025-09-08T12:56:22.498+0530[0m] {[34mscheduler_job_runner.py:[0m799} INFO[0m - Starting the scheduler[0m
[[34m2025-09-08T12:56:22.498+0530[0m] {[34mscheduler_job_runner.py:[0m806} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-09-08T12:56:22.501+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 705354[0m
[[34m2025-09-08T12:56:22.502+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T12:56:22.504+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T12:56:22.516+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T12:56:36.660+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T12:56:37.169+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:10:00+00:00 [scheduled]>[0m
[[34m2025-09-08T12:56:37.169+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T12:56:37.170+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:10:00+00:00 [scheduled]>[0m
[[34m2025-09-08T12:56:37.171+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:10:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T12:56:37.171+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T12:56:37.222+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T12:56:37.867+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T12:56:38.092+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T12:56:38.095+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T12:56:38.095+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T12:56:38.315+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:10:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T12:57:25.042+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T12:57:25.046+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T07:10:00+00:00, map_index=-1, run_start_date=2025-09-08 07:26:38.431582+00:00, run_end_date=2025-09-08 07:27:24.603304+00:00, run_duration=46.171722, state=success, executor_state=success, try_number=1, max_tries=1, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:26:37.170445+00:00, queued_by_job_id=15, pid=705433[0m
[[34m2025-09-08T12:57:25.396+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T12:57:25.530+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 07:10:00+00:00: scheduled__2025-09-08T07:10:00+00:00, state:running, queued_at: 2025-09-08 07:26:36.655063+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T12:57:25.531+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 07:10:00+00:00, run_id=scheduled__2025-09-08T07:10:00+00:00, run_start_date=2025-09-08 07:26:36.728128+00:00, run_end_date=2025-09-08 07:27:25.531169+00:00, run_duration=48.803041, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:10:00+00:00, data_interval_end=2025-09-08 07:20:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T12:57:25.536+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 07:20:00+00:00, run_after=2025-09-08 07:30:00+00:00[0m
[[34m2025-09-08T12:57:25.609+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:20:00+00:00 [scheduled]>[0m
[[34m2025-09-08T12:57:25.610+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T12:57:25.610+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:20:00+00:00 [scheduled]>[0m
[[34m2025-09-08T12:57:25.612+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:20:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T12:57:25.613+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T12:57:25.662+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T12:57:26.630+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T12:57:26.859+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T12:57:26.861+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T12:57:26.862+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T12:57:27.356+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:20:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T12:57:28.597+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T12:57:28.599+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:20:00+00:00, map_index=-1, run_start_date=2025-09-08 07:27:27.483486+00:00, run_end_date=2025-09-08 07:27:28.172304+00:00, run_duration=0.688818, state=success, executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:27:25.611181+00:00, queued_by_job_id=15, pid=706089[0m
[[34m2025-09-08T12:57:28.840+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:20:00+00:00: scheduled__2025-09-08T07:20:00+00:00, state:running, queued_at: 2025-09-08 07:27:25.392245+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T12:57:28.840+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:20:00+00:00, run_id=scheduled__2025-09-08T07:20:00+00:00, run_start_date=2025-09-08 07:27:25.462014+00:00, run_end_date=2025-09-08 07:27:28.840303+00:00, run_duration=3.378289, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:20:00+00:00, data_interval_end=2025-09-08 07:25:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T12:57:28.841+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:25:00+00:00, run_after=2025-09-08 07:30:00+00:00[0m
[[34m2025-09-08T13:00:01.676+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:00:01.678+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:00:01.907+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:20:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:25:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:00:01.908+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:00:01.908+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:00:01.908+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:20:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:25:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:00:01.910+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:20:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:00:01.910+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:00:01.910+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:25:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:00:01.910+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:00:01.955+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:00:02.655+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:00:02.871+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:00:02.873+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:00:02.874+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:00:03.102+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:20:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:00:48.319+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:00:48.986+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:00:49.209+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:00:49.211+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:00:49.212+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:00:49.437+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:25:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:00:50.277+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:00:50.278+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:00:50.283+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:25:00+00:00, map_index=-1, run_start_date=2025-09-08 07:30:49.515154+00:00, run_end_date=2025-09-08 07:30:49.895449+00:00, run_duration=0.380295, state=success, executor_state=success, try_number=1, max_tries=1, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:30:01.908920+00:00, queued_by_job_id=15, pid=707362[0m
[[34m2025-09-08T13:00:50.283+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T07:20:00+00:00, map_index=-1, run_start_date=2025-09-08 07:30:03.186075+00:00, run_end_date=2025-09-08 07:30:47.870604+00:00, run_duration=44.684529, state=success, executor_state=success, try_number=1, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:30:01.908920+00:00, queued_by_job_id=15, pid=706802[0m
[[34m2025-09-08T13:00:50.585+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 07:20:00+00:00: scheduled__2025-09-08T07:20:00+00:00, state:running, queued_at: 2025-09-08 07:30:01.674518+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:00:50.586+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 07:20:00+00:00, run_id=scheduled__2025-09-08T07:20:00+00:00, run_start_date=2025-09-08 07:30:01.774091+00:00, run_end_date=2025-09-08 07:30:50.586117+00:00, run_duration=48.812026, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:20:00+00:00, data_interval_end=2025-09-08 07:30:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:00:50.587+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 07:30:00+00:00, run_after=2025-09-08 07:40:00+00:00[0m
[[34m2025-09-08T13:00:50.589+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:25:00+00:00: scheduled__2025-09-08T07:25:00+00:00, state:running, queued_at: 2025-09-08 07:30:01.677585+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:00:50.589+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:25:00+00:00, run_id=scheduled__2025-09-08T07:25:00+00:00, run_start_date=2025-09-08 07:30:01.775405+00:00, run_end_date=2025-09-08 07:30:50.589165+00:00, run_duration=48.81376, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:25:00+00:00, data_interval_end=2025-09-08 07:30:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:00:50.590+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:30:00+00:00, run_after=2025-09-08 07:35:00+00:00[0m
[[34m2025-09-08T13:01:22.677+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:05:01.225+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:05:01.420+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:30:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:05:01.421+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:05:01.421+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:30:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:05:01.423+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:05:01.423+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:05:01.524+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:05:02.166+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:05:02.385+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:05:02.388+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:05:02.388+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:05:02.610+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:30:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:05:03.620+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:05:03.622+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:30:00+00:00, map_index=-1, run_start_date=2025-09-08 07:35:02.730674+00:00, run_end_date=2025-09-08 07:35:03.208508+00:00, run_duration=0.477834, state=success, executor_state=success, try_number=1, max_tries=1, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:35:01.422054+00:00, queued_by_job_id=15, pid=708669[0m
[[34m2025-09-08T13:05:03.924+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:30:00+00:00: scheduled__2025-09-08T07:30:00+00:00, state:running, queued_at: 2025-09-08 07:35:01.223911+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:05:03.924+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:30:00+00:00, run_id=scheduled__2025-09-08T07:30:00+00:00, run_start_date=2025-09-08 07:35:01.290278+00:00, run_end_date=2025-09-08 07:35:03.924219+00:00, run_duration=2.633941, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:30:00+00:00, data_interval_end=2025-09-08 07:35:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:05:03.925+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:35:00+00:00, run_after=2025-09-08 07:40:00+00:00[0m
[[34m2025-09-08T13:06:22.825+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:10:01.824+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:10:01.828+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:10:02.036+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:30:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:35:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:10:02.037+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:10:02.037+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:10:02.037+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:30:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:35:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:10:02.039+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:10:02.040+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:10:02.040+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:35:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:10:02.040+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:10:02.086+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:10:02.813+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:10:03.050+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:10:03.052+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:10:03.052+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:10:03.308+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:30:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:10:49.828+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:10:50.529+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:10:50.750+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:10:50.753+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:10:50.753+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:10:51.219+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:35:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:11:34.432+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:11:34.433+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:11:34.436+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:35:00+00:00, map_index=-1, run_start_date=2025-09-08 07:40:51.315562+00:00, run_end_date=2025-09-08 07:41:34.043477+00:00, run_duration=42.727915, state=success, executor_state=success, try_number=1, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:40:02.038673+00:00, queued_by_job_id=15, pid=710451[0m
[[34m2025-09-08T13:11:34.436+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T07:30:00+00:00, map_index=-1, run_start_date=2025-09-08 07:40:03.584274+00:00, run_end_date=2025-09-08 07:40:49.422571+00:00, run_duration=45.838297, state=success, executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:40:02.038673+00:00, queued_by_job_id=15, pid=709893[0m
[[34m2025-09-08T13:11:34.447+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=705354) last sent a heartbeat 92.64 seconds ago! Restarting it[0m
[[34m2025-09-08T13:11:34.453+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 705354. PIDs of all processes in the group: [705354][0m
[[34m2025-09-08T13:11:34.453+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 705354[0m
[[34m2025-09-08T13:11:34.545+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=705354, status='terminated', exitcode=0, started='12:56:22') (705354) terminated with exit code 0[0m
[[34m2025-09-08T13:11:34.551+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 710585[0m
[[34m2025-09-08T13:11:34.555+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T13:11:34.571+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T13:11:34.646+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:11:35.107+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 07:30:00+00:00: scheduled__2025-09-08T07:30:00+00:00, state:running, queued_at: 2025-09-08 07:40:01.826358+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:11:35.107+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 07:30:00+00:00, run_id=scheduled__2025-09-08T07:30:00+00:00, run_start_date=2025-09-08 07:40:01.899844+00:00, run_end_date=2025-09-08 07:41:35.107805+00:00, run_duration=93.207961, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:30:00+00:00, data_interval_end=2025-09-08 07:40:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:11:35.109+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 07:40:00+00:00, run_after=2025-09-08 07:50:00+00:00[0m
[[34m2025-09-08T13:11:35.110+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:35:00+00:00: scheduled__2025-09-08T07:35:00+00:00, state:running, queued_at: 2025-09-08 07:40:01.820164+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:11:35.111+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:35:00+00:00, run_id=scheduled__2025-09-08T07:35:00+00:00, run_start_date=2025-09-08 07:40:01.901182+00:00, run_end_date=2025-09-08 07:41:35.111124+00:00, run_duration=93.209942, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:35:00+00:00, data_interval_end=2025-09-08 07:40:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:11:35.112+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:40:00+00:00, run_after=2025-09-08 07:45:00+00:00[0m
[[34m2025-09-08T13:15:01.351+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:15:01.535+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:40:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:15:01.535+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:15:01.535+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:40:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:15:01.536+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:15:01.536+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:15:01.585+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:15:02.223+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:15:02.440+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:15:02.443+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:15:02.443+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:15:02.665+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:40:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:15:32.447+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:15:33.684+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:40:00+00:00, map_index=-1, run_start_date=2025-09-08 07:45:02.737047+00:00, run_end_date=2025-09-08 07:45:31.654681+00:00, run_duration=28.917634, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:45:01.535725+00:00, queued_by_job_id=15, pid=711456[0m
[[34m2025-09-08T13:15:34.846+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:40:00+00:00: scheduled__2025-09-08T07:40:00+00:00, state:running, queued_at: 2025-09-08 07:45:01.349566+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:15:34.846+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:40:00+00:00, run_id=scheduled__2025-09-08T07:40:00+00:00, run_start_date=2025-09-08 07:45:01.417463+00:00, run_end_date=2025-09-08 07:45:34.846499+00:00, run_duration=33.429036, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:40:00+00:00, data_interval_end=2025-09-08 07:45:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:15:34.847+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:45:00+00:00, run_after=2025-09-08 07:50:00+00:00[0m
[[34m2025-09-08T13:16:34.895+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:20:01.565+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:20:01.568+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:20:01.785+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:40:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:45:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:20:01.786+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:20:01.786+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:20:01.786+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:40:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:45:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:20:01.788+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:20:01.788+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:20:01.788+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:45:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:20:01.789+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:20:01.835+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:20:02.478+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:20:02.693+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:20:02.696+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:20:02.696+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:20:02.928+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:40:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:20:47.500+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:20:48.248+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:20:48.466+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:20:48.469+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:20:48.469+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:20:48.700+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:45:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:21:14.651+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:21:14.651+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:21:14.654+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:45:00+00:00, map_index=-1, run_start_date=2025-09-08 07:50:48.807966+00:00, run_end_date=2025-09-08 07:51:13.970040+00:00, run_duration=25.162074, state=success, executor_state=success, try_number=1, max_tries=1, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:50:01.787320+00:00, queued_by_job_id=15, pid=713160[0m
[[34m2025-09-08T13:21:14.654+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T07:40:00+00:00, map_index=-1, run_start_date=2025-09-08 07:50:03.015314+00:00, run_end_date=2025-09-08 07:50:46.925339+00:00, run_duration=43.910025, state=success, executor_state=success, try_number=1, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:50:01.787320+00:00, queued_by_job_id=15, pid=712606[0m
[[34m2025-09-08T13:21:14.665+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=710585) last sent a heartbeat 73.12 seconds ago! Restarting it[0m
[[34m2025-09-08T13:21:14.670+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 710585. PIDs of all processes in the group: [710585][0m
[[34m2025-09-08T13:21:14.670+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 710585[0m
[[34m2025-09-08T13:21:14.762+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=710585, status='terminated', exitcode=0, started='13:11:34') (710585) terminated with exit code 0[0m
[[34m2025-09-08T13:21:14.767+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 713264[0m
[[34m2025-09-08T13:21:14.772+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T13:21:14.785+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T13:21:15.377+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 07:40:00+00:00: scheduled__2025-09-08T07:40:00+00:00, state:running, queued_at: 2025-09-08 07:50:01.561170+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:21:15.378+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 07:40:00+00:00, run_id=scheduled__2025-09-08T07:40:00+00:00, run_start_date=2025-09-08 07:50:01.640735+00:00, run_end_date=2025-09-08 07:51:15.377983+00:00, run_duration=73.737248, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:40:00+00:00, data_interval_end=2025-09-08 07:50:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:21:15.379+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 07:50:00+00:00, run_after=2025-09-08 08:00:00+00:00[0m
[[34m2025-09-08T13:21:15.381+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:45:00+00:00: scheduled__2025-09-08T07:45:00+00:00, state:running, queued_at: 2025-09-08 07:50:01.566799+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:21:15.381+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:45:00+00:00, run_id=scheduled__2025-09-08T07:45:00+00:00, run_start_date=2025-09-08 07:50:01.642025+00:00, run_end_date=2025-09-08 07:51:15.381355+00:00, run_duration=73.73933, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:45:00+00:00, data_interval_end=2025-09-08 07:50:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:21:15.382+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:50:00+00:00, run_after=2025-09-08 07:55:00+00:00[0m
[[34m2025-09-08T13:21:35.244+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:25:01.126+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:25:01.323+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:50:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:25:01.323+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:25:01.323+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:50:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:25:01.325+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:50:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:25:01.326+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:25:01.376+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:25:02.042+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:25:02.276+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:25:02.279+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:25:02.279+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:25:02.530+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:50:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:25:32.327+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:25:32.330+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:50:00+00:00, map_index=-1, run_start_date=2025-09-08 07:55:02.622608+00:00, run_end_date=2025-09-08 07:55:31.596469+00:00, run_duration=28.973861, state=success, executor_state=success, try_number=1, max_tries=1, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:55:01.324420+00:00, queued_by_job_id=15, pid=714135[0m
[[34m2025-09-08T13:25:32.992+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:50:00+00:00: scheduled__2025-09-08T07:50:00+00:00, state:running, queued_at: 2025-09-08 07:55:01.124680+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:25:32.992+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:50:00+00:00, run_id=scheduled__2025-09-08T07:50:00+00:00, run_start_date=2025-09-08 07:55:01.205147+00:00, run_end_date=2025-09-08 07:55:32.992699+00:00, run_duration=31.787552, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:50:00+00:00, data_interval_end=2025-09-08 07:55:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:25:32.994+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 07:55:00+00:00, run_after=2025-09-08 08:00:00+00:00[0m
[[34m2025-09-08T13:25:39.578+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline manual__2025-09-08T07:55:37.477609+00:00 [scheduled]>[0m
[[34m2025-09-08T13:25:39.578+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:25:39.579+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline manual__2025-09-08T07:55:37.477609+00:00 [scheduled]>[0m
[[34m2025-09-08T13:25:39.581+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='manual__2025-09-08T07:55:37.477609+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:25:39.581+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'manual__2025-09-08T07:55:37.477609+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:25:39.764+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'manual__2025-09-08T07:55:37.477609+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:25:40.644+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:25:41.617+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:25:41.620+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:25:41.620+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:25:41.860+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline manual__2025-09-08T07:55:37.477609+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:26:52.767+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='manual__2025-09-08T07:55:37.477609+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:26:52.775+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=manual__2025-09-08T07:55:37.477609+00:00, map_index=-1, run_start_date=2025-09-08 07:55:42.120388+00:00, run_end_date=2025-09-08 07:56:52.167670+00:00, run_duration=70.047282, state=success, executor_state=success, try_number=1, max_tries=1, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 07:55:39.579857+00:00, queued_by_job_id=15, pid=714277[0m
[[34m2025-09-08T13:26:52.787+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=713264) last sent a heartbeat 73.85 seconds ago! Restarting it[0m
[[34m2025-09-08T13:26:52.801+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 713264. PIDs of all processes in the group: [713264][0m
[[34m2025-09-08T13:26:52.801+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 713264[0m
[[34m2025-09-08T13:26:53.868+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=713264, status='terminated', exitcode=0, started='13:21:14') (713264) terminated with exit code 0[0m
[[34m2025-09-08T13:26:53.874+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 715216[0m
[[34m2025-09-08T13:26:53.894+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[[34m2025-09-08T13:26:54.028+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-09-08T13:26:54.905+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T13:26:55.310+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 07:55:37.477609+00:00: manual__2025-09-08T07:55:37.477609+00:00, state:running, queued_at: 2025-09-08 07:55:37.485236+00:00. externally triggered: True> successful[0m
[[34m2025-09-08T13:26:55.311+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 07:55:37.477609+00:00, run_id=manual__2025-09-08T07:55:37.477609+00:00, run_start_date=2025-09-08 07:55:39.134801+00:00, run_end_date=2025-09-08 07:56:55.311186+00:00, run_duration=76.176385, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-08 07:45:37.477609+00:00, data_interval_end=2025-09-08 07:55:37.477609+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:30:01.036+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:30:01.039+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:30:01.270+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:50:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:55:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:30:01.270+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:30:01.270+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:30:01.271+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:50:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:55:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:30:01.273+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:50:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:30:01.273+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:30:01.274+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:55:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:30:01.274+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:30:01.328+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T07:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:30:07.613+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:30:08.162+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:30:08.165+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:30:08.165+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:30:08.402+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T07:50:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:30:58.962+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T07:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:31:00.397+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:31:00.614+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:31:00.617+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:31:00.617+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:31:00.889+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T07:55:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:31:33.928+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T07:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:31:33.929+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T07:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:31:33.932+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T07:55:00+00:00, map_index=-1, run_start_date=2025-09-08 08:01:00.985177+00:00, run_end_date=2025-09-08 08:01:33.027214+00:00, run_duration=32.042037, state=success, executor_state=success, try_number=1, max_tries=1, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:00:01.271928+00:00, queued_by_job_id=15, pid=716816[0m
[[34m2025-09-08T13:31:33.932+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T07:50:00+00:00, map_index=-1, run_start_date=2025-09-08 08:00:08.651927+00:00, run_end_date=2025-09-08 08:00:58.312552+00:00, run_duration=49.660625, state=success, executor_state=success, try_number=1, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:00:01.271928+00:00, queued_by_job_id=15, pid=716089[0m
[[34m2025-09-08T13:31:33.942+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=715216) last sent a heartbeat 92.92 seconds ago! Restarting it[0m
[[34m2025-09-08T13:31:33.948+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 715216. PIDs of all processes in the group: [715216][0m
[[34m2025-09-08T13:31:33.948+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 715216[0m
[[34m2025-09-08T13:31:34.040+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=715216, status='terminated', exitcode=0, started='13:26:53') (715216) terminated with exit code 0[0m
[[34m2025-09-08T13:31:34.043+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 716898[0m
[[34m2025-09-08T13:31:34.048+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T13:31:34.061+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T13:31:34.611+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 07:50:00+00:00: scheduled__2025-09-08T07:50:00+00:00, state:running, queued_at: 2025-09-08 08:00:01.037663+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:31:34.612+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 07:50:00+00:00, run_id=scheduled__2025-09-08T07:50:00+00:00, run_start_date=2025-09-08 08:00:01.121556+00:00, run_end_date=2025-09-08 08:01:34.612072+00:00, run_duration=93.490516, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:50:00+00:00, data_interval_end=2025-09-08 08:00:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:31:34.613+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 08:00:00+00:00, run_after=2025-09-08 08:10:00+00:00[0m
[[34m2025-09-08T13:31:34.615+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 07:55:00+00:00: scheduled__2025-09-08T07:55:00+00:00, state:running, queued_at: 2025-09-08 08:00:01.032316+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:31:34.615+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 07:55:00+00:00, run_id=scheduled__2025-09-08T07:55:00+00:00, run_start_date=2025-09-08 08:00:01.122540+00:00, run_end_date=2025-09-08 08:01:34.615562+00:00, run_duration=93.493022, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 07:55:00+00:00, data_interval_end=2025-09-08 08:00:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:31:34.616+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:00:00+00:00, run_after=2025-09-08 08:05:00+00:00[0m
[[34m2025-09-08T13:31:54.386+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:35:02.054+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:35:02.454+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:00:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:35:02.455+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:35:02.455+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:00:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:35:02.457+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:35:02.457+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:35:02.516+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:35:03.168+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:35:03.382+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:35:03.384+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:35:03.385+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:35:03.611+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:00:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:35:33.801+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:35:33.804+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:00:00+00:00, map_index=-1, run_start_date=2025-09-08 08:05:03.709383+00:00, run_end_date=2025-09-08 08:05:33.090021+00:00, run_duration=29.380638, state=success, executor_state=success, try_number=1, max_tries=1, job_id=30, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:05:02.456265+00:00, queued_by_job_id=15, pid=717772[0m
[[34m2025-09-08T13:35:34.324+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:00:00+00:00: scheduled__2025-09-08T08:00:00+00:00, state:running, queued_at: 2025-09-08 08:05:02.051256+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:35:34.324+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:00:00+00:00, run_id=scheduled__2025-09-08T08:00:00+00:00, run_start_date=2025-09-08 08:05:02.229382+00:00, run_end_date=2025-09-08 08:05:34.324442+00:00, run_duration=32.09506, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:00:00+00:00, data_interval_end=2025-09-08 08:05:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:35:34.325+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:05:00+00:00, run_after=2025-09-08 08:10:00+00:00[0m
[[34m2025-09-08T13:36:54.689+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:40:01.653+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:40:01.657+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:40:01.853+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:00:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:05:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:40:01.854+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:40:01.854+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:40:01.854+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:00:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:05:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:40:01.856+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:40:01.857+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:40:01.857+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:05:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:40:01.857+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:40:01.906+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:40:02.547+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:40:02.763+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:40:02.765+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:40:02.766+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:40:02.991+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:00:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:41:22.257+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:42:08.749+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:42:12.412+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:42:12.415+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:42:12.416+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:42:16.939+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:05:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:42:57.707+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:42:57.707+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:42:57.746+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:05:00+00:00, map_index=-1, run_start_date=2025-09-08 08:12:17.316501+00:00, run_end_date=2025-09-08 08:12:56.988241+00:00, run_duration=39.67174, state=success, executor_state=success, try_number=1, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:10:01.855611+00:00, queued_by_job_id=15, pid=720054[0m
[[34m2025-09-08T13:42:57.746+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T08:00:00+00:00, map_index=-1, run_start_date=2025-09-08 08:10:03.091466+00:00, run_end_date=2025-09-08 08:11:20.436896+00:00, run_duration=77.34543, state=success, executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:10:01.855611+00:00, queued_by_job_id=15, pid=719057[0m
[[34m2025-09-08T13:42:57.758+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=716898) last sent a heartbeat 176.12 seconds ago! Restarting it[0m
[[34m2025-09-08T13:42:57.773+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 716898. PIDs of all processes in the group: [716898][0m
[[34m2025-09-08T13:42:57.774+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 716898[0m
[[34m2025-09-08T13:42:58.790+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=716898, status='terminated', exitcode=0, started='13:31:33') (716898) terminated with exit code 0[0m
[[34m2025-09-08T13:42:58.796+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 720161[0m
[[34m2025-09-08T13:42:58.801+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T13:42:58.816+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T13:42:59.766+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:43:00.235+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 08:00:00+00:00: scheduled__2025-09-08T08:00:00+00:00, state:running, queued_at: 2025-09-08 08:10:01.650114+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:43:00.236+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 08:00:00+00:00, run_id=scheduled__2025-09-08T08:00:00+00:00, run_start_date=2025-09-08 08:10:01.733995+00:00, run_end_date=2025-09-08 08:13:00.235972+00:00, run_duration=178.501977, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:00:00+00:00, data_interval_end=2025-09-08 08:10:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:43:00.237+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 08:10:00+00:00, run_after=2025-09-08 08:20:00+00:00[0m
[[34m2025-09-08T13:43:00.239+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:05:00+00:00: scheduled__2025-09-08T08:05:00+00:00, state:running, queued_at: 2025-09-08 08:10:01.655323+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:43:00.239+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:05:00+00:00, run_id=scheduled__2025-09-08T08:05:00+00:00, run_start_date=2025-09-08 08:10:01.735370+00:00, run_end_date=2025-09-08 08:13:00.239322+00:00, run_duration=178.503952, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:05:00+00:00, data_interval_end=2025-09-08 08:10:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:43:00.240+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:10:00+00:00, run_after=2025-09-08 08:15:00+00:00[0m
[[34m2025-09-08T13:45:01.456+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:45:01.695+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:10:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:45:01.695+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:45:01.696+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:10:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:45:01.697+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:10:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:45:01.697+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:45:01.757+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:45:02.433+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:45:02.647+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:45:02.650+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:45:02.650+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:45:02.877+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:10:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:45:37.921+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:45:37.924+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:10:00+00:00, map_index=-1, run_start_date=2025-09-08 08:15:02.975607+00:00, run_end_date=2025-09-08 08:15:37.151456+00:00, run_duration=34.175849, state=success, executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:15:01.696599+00:00, queued_by_job_id=15, pid=720747[0m
[[34m2025-09-08T13:45:38.447+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:10:00+00:00: scheduled__2025-09-08T08:10:00+00:00, state:running, queued_at: 2025-09-08 08:15:01.454203+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:45:38.447+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:10:00+00:00, run_id=scheduled__2025-09-08T08:10:00+00:00, run_start_date=2025-09-08 08:15:01.550375+00:00, run_end_date=2025-09-08 08:15:38.447417+00:00, run_duration=36.897042, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:10:00+00:00, data_interval_end=2025-09-08 08:15:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:45:38.448+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:15:00+00:00, run_after=2025-09-08 08:20:00+00:00[0m
[[34m2025-09-08T13:47:59.986+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:50:01.284+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:50:01.288+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:50:01.530+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:10:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:15:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:50:01.530+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:50:01.530+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:50:01.531+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:10:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:15:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:50:01.533+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:10:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:50:01.533+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:50:01.533+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:15:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:50:01.534+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:50:01.591+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:50:02.244+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:50:02.457+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:50:02.460+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:50:02.460+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:50:02.703+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:10:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:51:44.040+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:52:31.959+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:52:40.166+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:52:40.177+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:52:40.177+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:52:47.366+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:15:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:53:34.947+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:53:34.947+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:15:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:53:34.951+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:15:00+00:00, map_index=-1, run_start_date=2025-09-08 08:22:48.280106+00:00, run_end_date=2025-09-08 08:23:34.301199+00:00, run_duration=46.021093, state=success, executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:20:01.531952+00:00, queued_by_job_id=15, pid=723021[0m
[[34m2025-09-08T13:53:34.952+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T08:10:00+00:00, map_index=-1, run_start_date=2025-09-08 08:20:02.809607+00:00, run_end_date=2025-09-08 08:21:42.626253+00:00, run_duration=99.816646, state=success, executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:20:01.531952+00:00, queued_by_job_id=15, pid=721834[0m
[[34m2025-09-08T13:53:34.962+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=720161) last sent a heartbeat 213.70 seconds ago! Restarting it[0m
[[34m2025-09-08T13:53:34.967+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 720161. PIDs of all processes in the group: [720161][0m
[[34m2025-09-08T13:53:34.968+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 720161[0m
[[34m2025-09-08T13:53:43.088+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=720161, status='terminated', exitcode=0, started='13:42:58') (720161) terminated with exit code 0[0m
[[34m2025-09-08T13:53:43.096+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 723128[0m
[[34m2025-09-08T13:53:43.570+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T13:53:44.812+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T13:53:49.463+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T13:53:52.486+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 08:10:00+00:00: scheduled__2025-09-08T08:10:00+00:00, state:running, queued_at: 2025-09-08 08:20:01.286265+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:53:52.486+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 08:10:00+00:00, run_id=scheduled__2025-09-08T08:10:00+00:00, run_start_date=2025-09-08 08:20:01.381361+00:00, run_end_date=2025-09-08 08:23:52.486488+00:00, run_duration=231.105127, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:10:00+00:00, data_interval_end=2025-09-08 08:20:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T13:53:52.488+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 08:20:00+00:00, run_after=2025-09-08 08:30:00+00:00[0m
[[34m2025-09-08T13:53:52.489+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:15:00+00:00: scheduled__2025-09-08T08:15:00+00:00, state:running, queued_at: 2025-09-08 08:20:01.279605+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:53:52.489+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:15:00+00:00, run_id=scheduled__2025-09-08T08:15:00+00:00, run_start_date=2025-09-08 08:20:01.381707+00:00, run_end_date=2025-09-08 08:23:52.489790+00:00, run_duration=231.108083, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:15:00+00:00, data_interval_end=2025-09-08 08:20:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:53:52.491+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:20:00+00:00, run_after=2025-09-08 08:25:00+00:00[0m
[[34m2025-09-08T13:55:01.794+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T13:55:02.454+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:20:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:55:02.454+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T13:55:02.454+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:20:00+00:00 [scheduled]>[0m
[[34m2025-09-08T13:55:02.455+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:20:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T13:55:02.455+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T13:55:02.602+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T13:55:03.391+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T13:55:03.608+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:55:03.610+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T13:55:03.611+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T13:55:03.826+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:20:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T13:55:48.662+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T13:55:48.664+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:20:00+00:00, map_index=-1, run_start_date=2025-09-08 08:25:04.020957+00:00, run_end_date=2025-09-08 08:25:47.497943+00:00, run_duration=43.476986, state=success, executor_state=success, try_number=1, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:25:02.455003+00:00, queued_by_job_id=15, pid=723381[0m
[[34m2025-09-08T13:55:49.416+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:20:00+00:00: scheduled__2025-09-08T08:20:00+00:00, state:running, queued_at: 2025-09-08 08:25:01.790151+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T13:55:49.416+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:20:00+00:00, run_id=scheduled__2025-09-08T08:20:00+00:00, run_start_date=2025-09-08 08:25:02.001244+00:00, run_end_date=2025-09-08 08:25:49.416603+00:00, run_duration=47.415359, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:20:00+00:00, data_interval_end=2025-09-08 08:25:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T13:55:49.417+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:25:00+00:00, run_after=2025-09-08 08:30:00+00:00[0m
[[34m2025-09-08T13:58:43.911+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:00:01.674+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:00:01.677+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:00:01.846+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:20:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:25:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:00:01.846+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:00:01.847+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:00:01.847+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:20:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:25:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:00:01.848+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:20:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:00:01.848+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:00:01.848+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:25:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:00:01.848+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:00:01.893+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:00:02.518+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:00:02.732+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:00:02.735+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:00:02.735+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:00:02.956+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:20:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[2025-09-08 14:03:27 +0530] [705351] [CRITICAL] WORKER TIMEOUT (pid:705352)
[2025-09-08 14:03:40 +0530] [705352] [INFO] Worker exiting (pid: 705352)
[2025-09-08 14:03:41 +0530] [705351] [ERROR] Worker (pid:705352) exited with code 1
[2025-09-08 14:03:41 +0530] [705351] [ERROR] Worker (pid:705352) exited with code 1.
[2025-09-08 14:03:43 +0530] [725212] [INFO] Booting worker with pid: 725212
[[34m2025-09-08T14:04:01.714+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:04:17.821+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:04:18.857+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:04:18.860+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:04:18.860+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:04:20.766+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:25:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:05:32.688+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:05:32.688+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:05:32.691+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:25:00+00:00, map_index=-1, run_start_date=2025-09-08 08:34:21.027587+00:00, run_end_date=2025-09-08 08:35:31.928743+00:00, run_duration=70.901156, state=success, executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:30:01.847441+00:00, queued_by_job_id=15, pid=725590[0m
[[34m2025-09-08T14:05:32.692+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T08:20:00+00:00, map_index=-1, run_start_date=2025-09-08 08:30:03.043103+00:00, run_end_date=2025-09-08 08:33:57.506580+00:00, run_duration=234.463477, state=success, executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:30:01.847441+00:00, queued_by_job_id=15, pid=724331[0m
[[34m2025-09-08T14:05:32.702+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=723128) last sent a heartbeat 331.04 seconds ago! Restarting it[0m
[[34m2025-09-08T14:05:32.708+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 723128. PIDs of all processes in the group: [723128][0m
[[34m2025-09-08T14:05:32.708+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 723128[0m
[[34m2025-09-08T14:05:34.379+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=723128, status='terminated', exitcode=0, started='13:53:42') (723128) terminated with exit code 0[0m
[[34m2025-09-08T14:05:34.385+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 725737[0m
[[34m2025-09-08T14:05:34.393+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:05:34.408+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:05:35.023+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:05:35.589+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 08:20:00+00:00: scheduled__2025-09-08T08:20:00+00:00, state:running, queued_at: 2025-09-08 08:30:01.675669+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:05:35.590+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 08:20:00+00:00, run_id=scheduled__2025-09-08T08:20:00+00:00, run_start_date=2025-09-08 08:30:01.746876+00:00, run_end_date=2025-09-08 08:35:35.589984+00:00, run_duration=333.843108, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:20:00+00:00, data_interval_end=2025-09-08 08:30:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T14:05:35.591+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 08:30:00+00:00, run_after=2025-09-08 08:40:00+00:00[0m
[[34m2025-09-08T14:05:35.593+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:25:00+00:00: scheduled__2025-09-08T08:25:00+00:00, state:running, queued_at: 2025-09-08 08:30:01.670074+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:05:35.593+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:25:00+00:00, run_id=scheduled__2025-09-08T08:25:00+00:00, run_start_date=2025-09-08 08:30:01.747225+00:00, run_end_date=2025-09-08 08:35:35.593339+00:00, run_duration=333.846114, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:25:00+00:00, data_interval_end=2025-09-08 08:30:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:05:35.594+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:30:00+00:00, run_after=2025-09-08 08:35:00+00:00[0m
[[34m2025-09-08T14:05:37.461+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:05:38.225+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:30:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:05:38.225+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:05:38.225+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:30:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:05:38.226+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:05:38.226+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:05:38.467+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:05:39.288+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:05:39.506+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:05:39.509+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:05:39.509+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:05:39.724+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:30:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:06:39.039+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:06:39.041+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:30:00+00:00, map_index=-1, run_start_date=2025-09-08 08:35:40.052265+00:00, run_end_date=2025-09-08 08:36:38.309284+00:00, run_duration=58.257019, state=success, executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:35:38.225626+00:00, queued_by_job_id=15, pid=725760[0m
[[34m2025-09-08T14:06:39.052+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=725737) last sent a heartbeat 61.61 seconds ago! Restarting it[0m
[[34m2025-09-08T14:06:39.057+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 725737. PIDs of all processes in the group: [725737][0m
[[34m2025-09-08T14:06:39.057+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 725737[0m
[[34m2025-09-08T14:06:39.150+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=725737, status='terminated', exitcode=0, started='14:05:33') (725737) terminated with exit code 0[0m
[[34m2025-09-08T14:06:39.156+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 725905[0m
[[34m2025-09-08T14:06:39.162+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:06:39.175+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:06:39.971+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:30:00+00:00: scheduled__2025-09-08T08:30:00+00:00, state:running, queued_at: 2025-09-08 08:35:37.457096+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:06:39.971+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:30:00+00:00, run_id=scheduled__2025-09-08T08:30:00+00:00, run_start_date=2025-09-08 08:35:37.727422+00:00, run_end_date=2025-09-08 08:36:39.971241+00:00, run_duration=62.243819, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:30:00+00:00, data_interval_end=2025-09-08 08:35:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:06:39.972+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:35:00+00:00, run_after=2025-09-08 08:40:00+00:00[0m
[[34m2025-09-08T14:10:01.790+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:10:01.794+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:10:02.387+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:30:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:35:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:10:02.387+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:10:02.387+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:10:02.387+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:30:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:35:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:10:02.388+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:10:02.388+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:10:02.388+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:35:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:10:02.388+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:10:02.579+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:10:03.395+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:10:03.608+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:10:03.611+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:10:03.611+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:10:03.825+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:30:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:12:42.618+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:12:47.157+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:12:47.531+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:12:47.533+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:12:47.533+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:12:47.847+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:35:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:13:47.271+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:13:47.272+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:13:47.274+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:35:00+00:00, map_index=-1, run_start_date=2025-09-08 08:42:48.397665+00:00, run_end_date=2025-09-08 08:43:46.387041+00:00, run_duration=57.989376, state=success, executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:40:02.387926+00:00, queued_by_job_id=15, pid=727597[0m
[[34m2025-09-08T14:13:47.274+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T08:30:00+00:00, map_index=-1, run_start_date=2025-09-08 08:40:04.022124+00:00, run_end_date=2025-09-08 08:42:41.407315+00:00, run_duration=157.385191, state=success, executor_state=success, try_number=1, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:40:02.387926+00:00, queued_by_job_id=15, pid=726581[0m
[[34m2025-09-08T14:13:47.285+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=725905) last sent a heartbeat 225.51 seconds ago! Restarting it[0m
[[34m2025-09-08T14:13:47.290+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 725905. PIDs of all processes in the group: [725905][0m
[[34m2025-09-08T14:13:47.290+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 725905[0m
[[34m2025-09-08T14:13:47.864+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=725905, status='terminated', exitcode=0, started='14:06:38') (725905) terminated with exit code 0[0m
[[34m2025-09-08T14:13:47.870+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 727722[0m
[[34m2025-09-08T14:13:47.895+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:13:48.106+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:13:48.185+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:13:48.617+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 08:30:00+00:00: scheduled__2025-09-08T08:30:00+00:00, state:running, queued_at: 2025-09-08 08:40:01.785959+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:13:48.617+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 08:30:00+00:00, run_id=scheduled__2025-09-08T08:30:00+00:00, run_start_date=2025-09-08 08:40:02.012635+00:00, run_end_date=2025-09-08 08:43:48.617700+00:00, run_duration=226.605065, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:30:00+00:00, data_interval_end=2025-09-08 08:40:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T14:13:48.619+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 08:40:00+00:00, run_after=2025-09-08 08:50:00+00:00[0m
[[34m2025-09-08T14:13:48.620+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:35:00+00:00: scheduled__2025-09-08T08:35:00+00:00, state:running, queued_at: 2025-09-08 08:40:01.792050+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:13:48.620+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:35:00+00:00, run_id=scheduled__2025-09-08T08:35:00+00:00, run_start_date=2025-09-08 08:40:02.012988+00:00, run_end_date=2025-09-08 08:43:48.620858+00:00, run_duration=226.60787, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:35:00+00:00, data_interval_end=2025-09-08 08:40:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:13:48.622+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:40:00+00:00, run_after=2025-09-08 08:45:00+00:00[0m
[[34m2025-09-08T14:15:01.246+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:15:01.486+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:40:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:15:01.486+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:15:01.486+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:40:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:15:01.489+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:15:01.489+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:15:01.547+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:15:02.173+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:15:02.389+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:15:02.392+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:15:02.392+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:15:02.624+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:40:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:15:52.282+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:15:52.285+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:40:00+00:00, map_index=-1, run_start_date=2025-09-08 08:45:02.721854+00:00, run_end_date=2025-09-08 08:45:51.600342+00:00, run_duration=48.878488, state=success, executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:45:01.487647+00:00, queued_by_job_id=15, pid=727989[0m
[[34m2025-09-08T14:15:52.296+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=727722) last sent a heartbeat 51.06 seconds ago! Restarting it[0m
[[34m2025-09-08T14:15:52.301+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 727722. PIDs of all processes in the group: [727722][0m
[[34m2025-09-08T14:15:52.301+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 727722[0m
[[34m2025-09-08T14:15:52.393+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=727722, status='terminated', exitcode=0, started='14:13:47') (727722) terminated with exit code 0[0m
[[34m2025-09-08T14:15:52.399+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 728060[0m
[[34m2025-09-08T14:15:52.406+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:15:52.419+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:15:52.968+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:40:00+00:00: scheduled__2025-09-08T08:40:00+00:00, state:running, queued_at: 2025-09-08 08:45:01.244974+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:15:52.968+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:40:00+00:00, run_id=scheduled__2025-09-08T08:40:00+00:00, run_start_date=2025-09-08 08:45:01.339051+00:00, run_end_date=2025-09-08 08:45:52.968808+00:00, run_duration=51.629757, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:40:00+00:00, data_interval_end=2025-09-08 08:45:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:15:52.970+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:45:00+00:00, run_after=2025-09-08 08:50:00+00:00[0m
[[34m2025-09-08T14:18:48.423+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:20:02.015+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:20:02.017+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:20:02.249+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:40:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:45:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:20:02.249+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:20:02.250+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:20:02.250+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:40:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:45:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:20:02.251+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:20:02.251+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:20:02.251+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:45:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:20:02.251+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:20:02.303+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:20:02.929+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:20:03.142+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:20:03.145+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:20:03.145+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:20:03.359+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:40:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:21:36.907+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:21:41.610+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:21:42.112+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:21:42.115+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:21:42.115+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:21:44.420+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:45:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:23:12.307+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:23:12.307+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:23:12.310+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:45:00+00:00, map_index=-1, run_start_date=2025-09-08 08:51:44.865525+00:00, run_end_date=2025-09-08 08:53:11.488982+00:00, run_duration=86.623457, state=success, executor_state=success, try_number=1, max_tries=1, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:50:02.250451+00:00, queued_by_job_id=15, pid=729759[0m
[[34m2025-09-08T14:23:12.310+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T08:40:00+00:00, map_index=-1, run_start_date=2025-09-08 08:50:03.451713+00:00, run_end_date=2025-09-08 08:51:36.028358+00:00, run_duration=92.576645, state=success, executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:50:02.250451+00:00, queued_by_job_id=15, pid=728859[0m
[[34m2025-09-08T14:23:12.320+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=728060) last sent a heartbeat 190.31 seconds ago! Restarting it[0m
[[34m2025-09-08T14:23:12.325+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 728060. PIDs of all processes in the group: [728060][0m
[[34m2025-09-08T14:23:12.325+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 728060[0m
[[34m2025-09-08T14:23:12.418+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=728060, status='terminated', exitcode=0, started='14:15:52') (728060) terminated with exit code 0[0m
[[34m2025-09-08T14:23:12.420+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 730004[0m
[[34m2025-09-08T14:23:12.423+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:23:12.435+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:23:13.025+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 08:40:00+00:00: scheduled__2025-09-08T08:40:00+00:00, state:running, queued_at: 2025-09-08 08:50:02.013804+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:23:13.025+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 08:40:00+00:00, run_id=scheduled__2025-09-08T08:40:00+00:00, run_start_date=2025-09-08 08:50:02.088801+00:00, run_end_date=2025-09-08 08:53:13.025326+00:00, run_duration=190.936525, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:40:00+00:00, data_interval_end=2025-09-08 08:50:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T14:23:13.026+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 08:50:00+00:00, run_after=2025-09-08 09:00:00+00:00[0m
[[34m2025-09-08T14:23:13.028+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:45:00+00:00: scheduled__2025-09-08T08:45:00+00:00, state:running, queued_at: 2025-09-08 08:50:02.016252+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:23:13.028+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:45:00+00:00, run_id=scheduled__2025-09-08T08:45:00+00:00, run_start_date=2025-09-08 08:50:02.089737+00:00, run_end_date=2025-09-08 08:53:13.028482+00:00, run_duration=190.938745, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:45:00+00:00, data_interval_end=2025-09-08 08:50:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:23:13.029+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:50:00+00:00, run_after=2025-09-08 08:55:00+00:00[0m
[[34m2025-09-08T14:23:48.773+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:25:01.976+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:25:02.777+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:50:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:25:02.777+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:25:02.777+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:50:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:25:02.778+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:50:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:25:02.778+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:25:03.086+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:25:03.936+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:25:04.150+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:25:04.153+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:25:04.153+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:25:04.395+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:50:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:26:08.525+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:26:08.528+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:50:00+00:00, map_index=-1, run_start_date=2025-09-08 08:55:04.754791+00:00, run_end_date=2025-09-08 08:56:07.697244+00:00, run_duration=62.942453, state=success, executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 08:55:02.778013+00:00, queued_by_job_id=15, pid=730323[0m
[[34m2025-09-08T14:26:08.538+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=730004) last sent a heartbeat 66.58 seconds ago! Restarting it[0m
[[34m2025-09-08T14:26:08.543+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 730004. PIDs of all processes in the group: [730004][0m
[[34m2025-09-08T14:26:08.543+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 730004[0m
[[34m2025-09-08T14:26:08.636+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=730004, status='terminated', exitcode=0, started='14:23:12') (730004) terminated with exit code 0[0m
[[34m2025-09-08T14:26:08.642+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 730469[0m
[[34m2025-09-08T14:26:08.646+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:26:08.658+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:26:09.334+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:50:00+00:00: scheduled__2025-09-08T08:50:00+00:00, state:running, queued_at: 2025-09-08 08:55:01.972484+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:26:09.335+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:50:00+00:00, run_id=scheduled__2025-09-08T08:50:00+00:00, run_start_date=2025-09-08 08:55:02.254357+00:00, run_end_date=2025-09-08 08:56:09.335062+00:00, run_duration=67.080705, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:50:00+00:00, data_interval_end=2025-09-08 08:55:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:26:09.336+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 08:55:00+00:00, run_after=2025-09-08 09:00:00+00:00[0m
[[34m2025-09-08T14:28:49.123+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:30:01.254+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:30:01.260+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:30:02.181+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:50:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:55:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:30:02.181+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:30:02.181+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:30:02.181+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:50:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:55:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:30:02.182+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:50:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:30:02.182+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:30:02.182+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:55:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:30:02.182+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:30:02.423+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T08:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:30:03.242+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:30:03.453+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:30:03.456+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:30:03.456+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:30:03.745+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T08:50:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:40:04.123+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T08:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:40:43.936+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:40:51.654+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:40:51.670+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:40:51.670+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:40:58.830+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T08:55:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:42:17.339+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T08:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:42:17.339+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T08:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:42:17.342+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T08:55:00+00:00, map_index=-1, run_start_date=2025-09-08 09:10:59.979541+00:00, run_end_date=2025-09-08 09:12:16.600649+00:00, run_duration=76.621108, state=success, executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:00:02.181844+00:00, queued_by_job_id=15, pid=733405[0m
[[34m2025-09-08T14:42:17.342+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T08:50:00+00:00, map_index=-1, run_start_date=2025-09-08 09:00:04.008976+00:00, run_end_date=2025-09-08 09:10:03.309115+00:00, run_duration=599.300139, state=success, executor_state=success, try_number=1, max_tries=1, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:00:02.181844+00:00, queued_by_job_id=15, pid=731158[0m
[[34m2025-09-08T14:42:17.353+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=730469) last sent a heartbeat 736.12 seconds ago! Restarting it[0m
[[34m2025-09-08T14:42:17.358+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 730469. PIDs of all processes in the group: [730469][0m
[[34m2025-09-08T14:42:17.358+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 730469[0m
[[34m2025-09-08T14:42:19.028+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=730469, status='terminated', exitcode=0, started='14:26:08') (730469) terminated with exit code 0[0m
[[34m2025-09-08T14:42:19.035+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 733621[0m
[[34m2025-09-08T14:42:19.043+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:42:19.056+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:42:19.320+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:42:19.855+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 08:50:00+00:00: scheduled__2025-09-08T08:50:00+00:00, state:running, queued_at: 2025-09-08 09:00:01.248947+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:42:19.855+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 08:50:00+00:00, run_id=scheduled__2025-09-08T08:50:00+00:00, run_start_date=2025-09-08 09:00:01.537629+00:00, run_end_date=2025-09-08 09:12:19.855612+00:00, run_duration=738.317983, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:50:00+00:00, data_interval_end=2025-09-08 09:00:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T14:42:19.857+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 09:00:00+00:00, run_after=2025-09-08 09:10:00+00:00[0m
[[34m2025-09-08T14:42:19.858+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 08:55:00+00:00: scheduled__2025-09-08T08:55:00+00:00, state:running, queued_at: 2025-09-08 09:00:01.257342+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:42:19.858+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 08:55:00+00:00, run_id=scheduled__2025-09-08T08:55:00+00:00, run_start_date=2025-09-08 09:00:01.537980+00:00, run_end_date=2025-09-08 09:12:19.858810+00:00, run_duration=738.32083, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 08:55:00+00:00, data_interval_end=2025-09-08 09:00:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:42:19.860+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 09:05:00+00:00, run_after=2025-09-08 09:10:00+00:00[0m
[[34m2025-09-08T14:42:21.658+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:42:21.661+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:42:22.544+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T09:00:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:05:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:42:22.544+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:42:22.544+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:42:22.545+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T09:00:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:05:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:42:22.546+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T09:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:42:22.546+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T09:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:42:22.546+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:05:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:42:22.546+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:42:22.795+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T09:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:42:23.597+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:42:23.810+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:42:23.812+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:42:23.812+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:42:24.041+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T09:00:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:45:27.299+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:45:56.191+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:46:00.914+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:46:00.936+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:46:00.936+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:46:01.397+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:05:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:47:03.288+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T09:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:47:03.289+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:47:03.292+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T09:05:00+00:00, map_index=-1, run_start_date=2025-09-08 09:16:02.273120+00:00, run_end_date=2025-09-08 09:17:02.431442+00:00, run_duration=60.158322, state=success, executor_state=success, try_number=1, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:12:22.545390+00:00, queued_by_job_id=15, pid=734969[0m
[[34m2025-09-08T14:47:03.292+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T09:00:00+00:00, map_index=-1, run_start_date=2025-09-08 09:12:24.357022+00:00, run_end_date=2025-09-08 09:15:26.432864+00:00, run_duration=182.075842, state=success, executor_state=success, try_number=1, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:12:22.545390+00:00, queued_by_job_id=15, pid=733646[0m
[[34m2025-09-08T14:47:03.302+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=733621) last sent a heartbeat 281.66 seconds ago! Restarting it[0m
[[34m2025-09-08T14:47:03.308+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 733621. PIDs of all processes in the group: [733621][0m
[[34m2025-09-08T14:47:03.308+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 733621[0m
[[34m2025-09-08T14:47:03.962+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=733621, status='terminated', exitcode=0, started='14:42:18') (733621) terminated with exit code 0[0m
[[34m2025-09-08T14:47:03.969+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 735074[0m
[[34m2025-09-08T14:47:03.977+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:47:03.990+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:47:04.622+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 09:00:00+00:00: scheduled__2025-09-08T09:00:00+00:00, state:running, queued_at: 2025-09-08 09:12:21.659570+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:47:04.622+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 09:00:00+00:00, run_id=scheduled__2025-09-08T09:00:00+00:00, run_start_date=2025-09-08 09:12:21.901620+00:00, run_end_date=2025-09-08 09:17:04.622909+00:00, run_duration=282.721289, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 09:00:00+00:00, data_interval_end=2025-09-08 09:10:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T14:47:04.624+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 09:10:00+00:00, run_after=2025-09-08 09:20:00+00:00[0m
[[34m2025-09-08T14:47:04.626+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 09:05:00+00:00: scheduled__2025-09-08T09:05:00+00:00, state:running, queued_at: 2025-09-08 09:12:21.653402+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:47:04.626+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 09:05:00+00:00, run_id=scheduled__2025-09-08T09:05:00+00:00, run_start_date=2025-09-08 09:12:21.901975+00:00, run_end_date=2025-09-08 09:17:04.626292+00:00, run_duration=282.724317, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 09:05:00+00:00, data_interval_end=2025-09-08 09:10:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:47:04.627+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 09:10:00+00:00, run_after=2025-09-08 09:15:00+00:00[0m
[[34m2025-09-08T14:47:06.329+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:47:07.082+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:10:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:47:07.082+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:47:07.082+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:10:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:47:07.083+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:10:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:47:07.083+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:47:07.325+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:47:08.160+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:47:08.381+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:47:08.384+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:47:08.384+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:47:08.745+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:10:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:48:09.166+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:48:09.168+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T09:10:00+00:00, map_index=-1, run_start_date=2025-09-08 09:17:09.002282+00:00, run_end_date=2025-09-08 09:18:07.404712+00:00, run_duration=58.40243, state=success, executor_state=success, try_number=1, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:17:07.083099+00:00, queued_by_job_id=15, pid=735098[0m
[[34m2025-09-08T14:48:09.179+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=735074) last sent a heartbeat 62.87 seconds ago! Restarting it[0m
[[34m2025-09-08T14:48:09.183+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 735074. PIDs of all processes in the group: [735074][0m
[[34m2025-09-08T14:48:09.183+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 735074[0m
[[34m2025-09-08T14:48:09.275+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=735074, status='terminated', exitcode=0, started='14:47:03') (735074) terminated with exit code 0[0m
[[34m2025-09-08T14:48:09.278+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 735202[0m
[[34m2025-09-08T14:48:09.282+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:48:09.295+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:48:09.626+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:48:10.074+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 09:10:00+00:00: scheduled__2025-09-08T09:10:00+00:00, state:running, queued_at: 2025-09-08 09:17:06.325739+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:48:10.074+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 09:10:00+00:00, run_id=scheduled__2025-09-08T09:10:00+00:00, run_start_date=2025-09-08 09:17:06.603497+00:00, run_end_date=2025-09-08 09:18:10.074645+00:00, run_duration=63.471148, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 09:10:00+00:00, data_interval_end=2025-09-08 09:15:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:48:10.076+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 09:15:00+00:00, run_after=2025-09-08 09:20:00+00:00[0m
[[34m2025-09-08T14:50:01.651+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:50:01.655+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG ecom_etl_10min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:50:02.401+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T09:10:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:15:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:50:02.401+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecom_etl_10min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:50:02.401+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:50:02.401+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T09:10:00+00:00 [scheduled]>
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:15:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:50:02.402+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T09:10:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:50:02.402+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T09:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:50:02.402+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:15:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:50:02.403+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:50:02.669+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecom_etl_10min', 'run_etl_pipeline', 'scheduled__2025-09-08T09:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:50:03.432+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:50:03.645+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:50:03.648+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:50:03.648+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:50:03.905+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecom_etl_10min.run_etl_pipeline scheduled__2025-09-08T09:10:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:53:42.761+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:55:05.822+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:55:18.263+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:55:18.295+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:55:18.296+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:55:19.447+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:15:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:57:26.155+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecom_etl_10min', task_id='run_etl_pipeline', run_id='scheduled__2025-09-08T09:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:57:26.155+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:15:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:57:26.158+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T09:15:00+00:00, map_index=-1, run_start_date=2025-09-08 09:25:21.288395+00:00, run_end_date=2025-09-08 09:27:25.503170+00:00, run_duration=124.214775, state=success, executor_state=success, try_number=1, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:20:02.401995+00:00, queued_by_job_id=15, pid=737342[0m
[[34m2025-09-08T14:57:26.158+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecom_etl_10min, task_id=run_etl_pipeline, run_id=scheduled__2025-09-08T09:10:00+00:00, map_index=-1, run_start_date=2025-09-08 09:20:04.165698+00:00, run_end_date=2025-09-08 09:23:37.738466+00:00, run_duration=213.572768, state=success, executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:20:02.401995+00:00, queued_by_job_id=15, pid=735619[0m
[[34m2025-09-08T14:57:26.169+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=735202) last sent a heartbeat 444.52 seconds ago! Restarting it[0m
[[34m2025-09-08T14:57:26.174+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 735202. PIDs of all processes in the group: [735202][0m
[[34m2025-09-08T14:57:26.174+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 735202[0m
[[34m2025-09-08T14:57:27.763+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=735202, status='terminated', exitcode=0, started='14:48:08') (735202) terminated with exit code 0[0m
[[34m2025-09-08T14:57:27.769+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 737599[0m
[[34m2025-09-08T14:57:27.774+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:57:27.788+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:57:27.987+0530[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-08T14:57:28.528+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun ecom_etl_10min @ 2025-09-08 09:10:00+00:00: scheduled__2025-09-08T09:10:00+00:00, state:running, queued_at: 2025-09-08 09:20:01.652901+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:57:28.529+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=ecom_etl_10min, execution_date=2025-09-08 09:10:00+00:00, run_id=scheduled__2025-09-08T09:10:00+00:00, run_start_date=2025-09-08 09:20:01.908286+00:00, run_end_date=2025-09-08 09:27:28.528944+00:00, run_duration=446.620658, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 09:10:00+00:00, data_interval_end=2025-09-08 09:20:00+00:00, dag_hash=b1abdcffe565f7a52eaf4e42974626a6[0m
[[34m2025-09-08T14:57:28.530+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for ecom_etl_10min to 2025-09-08 09:20:00+00:00, run_after=2025-09-08 09:30:00+00:00[0m
[[34m2025-09-08T14:57:28.532+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 09:15:00+00:00: scheduled__2025-09-08T09:15:00+00:00, state:running, queued_at: 2025-09-08 09:20:01.649477+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:57:28.532+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 09:15:00+00:00, run_id=scheduled__2025-09-08T09:15:00+00:00, run_start_date=2025-09-08 09:20:01.908647+00:00, run_end_date=2025-09-08 09:27:28.532271+00:00, run_duration=446.623624, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 09:15:00+00:00, data_interval_end=2025-09-08 09:20:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:57:28.533+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 09:20:00+00:00, run_after=2025-09-08 09:25:00+00:00[0m
[[34m2025-09-08T14:57:30.076+0530[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG synth_stream_5min is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-09-08T14:57:30.785+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:20:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:57:30.785+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG synth_stream_5min has 0/16 running and queued tasks[0m
[[34m2025-09-08T14:57:30.785+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:20:00+00:00 [scheduled]>[0m
[[34m2025-09-08T14:57:30.786+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:20:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-08T14:57:30.786+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
[[34m2025-09-08T14:57:30.981+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'synth_stream_5min', 'run_synth_stream', 'scheduled__2025-09-08T09:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecom_schedules.py'][0m
/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/configuration.py:727 DeprecationWarning: The log_filename_template option in [core] has been moved to the log_filename_template option in [logging] - the old setting has been used, but please update your config.
[[34m2025-09-08T14:57:31.750+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/dags/ecom_schedules.py[0m
[[34m2025-09-08T14:57:31.975+0530[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:57:31.978+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-08T14:57:31.978+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-08T14:57:32.193+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: synth_stream_5min.run_synth_stream scheduled__2025-09-08T09:20:00+00:00 [queued]> on host softsuave-ASUS-EXPERTCENTER-D700ME-D500ME[0m
[[34m2025-09-08T14:58:42.228+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='synth_stream_5min', task_id='run_synth_stream', run_id='scheduled__2025-09-08T09:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-08T14:58:42.230+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=synth_stream_5min, task_id=run_synth_stream, run_id=scheduled__2025-09-08T09:20:00+00:00, map_index=-1, run_start_date=2025-09-08 09:27:32.460717+00:00, run_end_date=2025-09-08 09:28:41.279824+00:00, run_duration=68.819107, state=success, executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-08 09:27:30.785630+00:00, queued_by_job_id=15, pid=737628[0m
[[34m2025-09-08T14:58:42.241+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=737599) last sent a heartbeat 72.17 seconds ago! Restarting it[0m
[[34m2025-09-08T14:58:42.246+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 737599. PIDs of all processes in the group: [737599][0m
[[34m2025-09-08T14:58:42.246+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 737599[0m
[[34m2025-09-08T14:58:42.338+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=737599, status='terminated', exitcode=0, started='14:57:27') (737599) terminated with exit code 0[0m
[[34m2025-09-08T14:58:42.344+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 737798[0m
[[34m2025-09-08T14:58:42.348+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-08T14:58:42.361+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-08T14:58:43.095+0530[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun synth_stream_5min @ 2025-09-08 09:20:00+00:00: scheduled__2025-09-08T09:20:00+00:00, state:running, queued_at: 2025-09-08 09:27:30.074532+00:00. externally triggered: False> successful[0m
[[34m2025-09-08T14:58:43.096+0530[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=synth_stream_5min, execution_date=2025-09-08 09:20:00+00:00, run_id=scheduled__2025-09-08T09:20:00+00:00, run_start_date=2025-09-08 09:27:30.280980+00:00, run_end_date=2025-09-08 09:28:43.096232+00:00, run_duration=72.815252, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-08 09:20:00+00:00, data_interval_end=2025-09-08 09:25:00+00:00, dag_hash=4bfdb6e481d0e65badbf93f627e77e22[0m
[[34m2025-09-08T14:58:43.098+0530[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for synth_stream_5min to 2025-09-08 09:25:00+00:00, run_after=2025-09-08 09:30:00+00:00[0m
