{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c99efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark import StorageLevel\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523c76b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/06 18:56:44 WARN Utils: Your hostname, softsuave-ASUS-EXPERTCENTER-D700ME-D500ME resolves to a loopback address: 127.0.1.1; using 192.168.6.3 instead (on interface eno2)\n",
      "25/09/06 18:56:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/06 18:56:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/06 18:56:45 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark JAR config: /media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/Jars/mssql-jdbc-12.2.0.jre8.jar\n",
      "Spark Version: 3.4.2\n",
      "✅ JDBC driver loaded\n"
     ]
    }
   ],
   "source": [
    "jar_path = \"/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/Jars/mssql-jdbc-12.2.0.jre8.jar\"\n",
    "\n",
    "try:\n",
    "    spark.stop()  # pyright: ignore[reportUndefinedVariable]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ECommerce_ETL_Database_Storage\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.dynamicPartitionPruning.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.cbo.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Verify\n",
    "print(\"✅ Spark JAR config:\", spark.sparkContext.getConf().get(\"spark.jars\"))\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"✅ JDBC driver loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e5a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODBC_CONFIG = {\n",
    "    \"source_db\": {\n",
    "        \"server\": \"localhost\",\n",
    "        \"database\": \"ecom_db\",\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Gova#ss123\",\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\"\n",
    "    },\n",
    "    \"target_db\": {\n",
    "        \"server\": \"localhost\", \n",
    "        \"database\": \"ecom_analytics\",\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Gova#ss123\",\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e07698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_odbc_url(config, database_override=None):\n",
    "    \"\"\"Build ODBC connection string for pyodbc\"\"\"\n",
    "    db_name = database_override if database_override else config[\"database\"]\n",
    "    \n",
    "    odbc_string = (\n",
    "        f\"DRIVER={{{config['driver']}}};\"\n",
    "        f\"SERVER={config['server']};\"\n",
    "        f\"DATABASE={db_name};\"\n",
    "        f\"UID={config['user']};\"\n",
    "        f\"PWD={config['password']};\"\n",
    "        f\"Encrypt=yes;\"\n",
    "        f\"TrustServerCertificate=yes;\"\n",
    "        f\"Connection Timeout=30;\"\n",
    "    )\n",
    "    \n",
    "    return odbc_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b274cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_ODBC_CONN = {\n",
    "    \"server\": ODBC_CONFIG[\"source_db\"][\"server\"],\n",
    "    \"database\": ODBC_CONFIG[\"source_db\"][\"database\"],\n",
    "    \"user\": ODBC_CONFIG[\"source_db\"][\"user\"],\n",
    "    \"password\": ODBC_CONFIG[\"source_db\"][\"password\"],\n",
    "    \"driver\": \"ODBC Driver 17 for SQL Server\",  # <-- ODBC driver\n",
    "    \"encrypt\": \"yes\",\n",
    "    \"trustServerCertificate\": \"yes\",\n",
    "    \"timeout\": 30\n",
    "}\n",
    "\n",
    "TARGET_ODBC_CONN = {\n",
    "    \"server\": ODBC_CONFIG[\"target_db\"][\"server\"],\n",
    "    \"database\": ODBC_CONFIG[\"target_db\"][\"database\"],\n",
    "    \"user\": ODBC_CONFIG[\"target_db\"][\"user\"],\n",
    "    \"password\": ODBC_CONFIG[\"target_db\"][\"password\"],\n",
    "    \"driver\": \"ODBC Driver 17 for SQL Server\",  # <-- ODBC driver\n",
    "    \"encrypt\": \"yes\",\n",
    "    \"trustServerCertificate\": \"yes\",\n",
    "    \"timeout\": 30\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07872c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odbc_connection(config, database_override=None, autocommit=False):\n",
    "    \"\"\"Get direct ODBC connection for DDL operations\"\"\"\n",
    "    db_name = database_override if database_override else config[\"database\"]\n",
    "    \n",
    "    connection_string = (\n",
    "        f\"DRIVER={{{config['driver']}}};\"\n",
    "        f\"SERVER={config['server']};\"\n",
    "        f\"DATABASE={db_name};\"\n",
    "        f\"UID={config['user']};\"\n",
    "        f\"PWD={config['password']};\"\n",
    "        f\"Encrypt=yes;\"\n",
    "        f\"TrustServerCertificate=yes;\"\n",
    "    )\n",
    "    \n",
    "    return pyodbc.connect(connection_string, autocommit=autocommit)\n",
    "\n",
    "# URLs for Spark JDBC operations\n",
    "SOURCE_URL = build_odbc_url(ODBC_CONFIG[\"source_db\"])\n",
    "TARGET_URL = build_odbc_url(ODBC_CONFIG[\"target_db\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a533b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading customers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded customers: 1,000 records\n",
      "Loading orders...\n",
      "✅ Loaded orders: 5,000 records\n",
      "Loading order_items...\n",
      "✅ Loaded order_items: 8,716 records\n",
      "Loading products...\n",
      "✅ Loaded products: 500 records\n",
      "Loading inventory...\n",
      "✅ Loaded inventory: 500 records\n",
      "Loading clickstream...\n",
      "✅ Loaded clickstream: 47,851 records\n",
      "RAW TABLES {'customers': DataFrame[customer_id: int, first_name: string, last_name: string, email: string, phone: string, country: string, state: string, city: string, postal_code: string, street: string, signup_date: timestamp, last_login: timestamp, segment: string, lifetime_value: decimal(14,2)], 'orders': DataFrame[order_id: int, order_uuid: string, customer_id: int, order_date: timestamp, status: string, payment_method: string, subtotal: decimal(12,2), shipping: decimal(10,2), tax: decimal(10,2), discount: decimal(10,2), total_amount: decimal(12,2), placed_via: string], 'order_items': DataFrame[order_item_id: int, order_id: int, product_id: int, sku: string, quantity: int, unit_price: decimal(10,2), line_total: decimal(12,2)], 'products': DataFrame[product_id: int, sku: string, product_name: string, category: string, brand: string, description: string, price: decimal(10,2), rrp: decimal(10,2), created_at: timestamp, popularity: int], 'inventory': DataFrame[inventory_id: int, product_id: int, stock_level: int, reorder_threshold: int, last_restocked: timestamp], 'clickstream': DataFrame[click_id: bigint, customer_id: int, session_id: string, page: string, page_url: string, product_id: int, timestamp: timestamp, device: string, user_agent: string, referrer: string, utm_source: string, ip_address: string]}\n",
      "✅ All tables loaded as Spark DataFrames\n"
     ]
    }
   ],
   "source": [
    "# PySpark Data Loading Functions\n",
    "def load_spark_tables():\n",
    "    \"\"\"Load all tables as Spark DataFrames using JDBC\"\"\"\n",
    "    \n",
    "    # JDBC connection properties\n",
    "    jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=ecom_db;encrypt=true;trustServerCertificate=true\"\n",
    "    connection_properties = {\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Gova#ss123\",\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    }\n",
    "    \n",
    "    tables = {}\n",
    "    table_names = [\"customers\", \"orders\", \"order_items\", \"products\", \"inventory\", \"clickstream\"]\n",
    "    \n",
    "    for table_name in table_names:\n",
    "        try:\n",
    "            print(f\"Loading {table_name}...\")\n",
    "            df = spark.read.jdbc(jdbc_url, table_name, properties=connection_properties)\n",
    "            tables[table_name] = df\n",
    "            print(f\"✅ Loaded {table_name}: {df.count():,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {table_name}: {e}\")\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# Load all data as Spark DataFrames\n",
    "raw_tables = load_spark_tables()\n",
    "print(\"RAW TABLES\", raw_tables)\n",
    "\n",
    "# Extract individual DataFrames\n",
    "customers = raw_tables[\"customers\"]\n",
    "orders = raw_tables[\"orders\"] \n",
    "order_items = raw_tables[\"order_items\"]\n",
    "products = raw_tables[\"products\"]\n",
    "inventory = raw_tables[\"inventory\"]\n",
    "clickstream = raw_tables[\"clickstream\"]\n",
    "\n",
    "print(\"✅ All tables loaded as Spark DataFrames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0aa33e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating RFM Analysis...\n",
      "+-----------+--------------------+--------+---------+-------+-------+-------+-------+---------------+\n",
      "|customer_id|     last_order_date|monetary|frequency|recency|r_score|f_score|m_score|    rfm_segment|\n",
      "+-----------+--------------------+--------+---------+-------+-------+-------+-------+---------------+\n",
      "|        881|2025-05-13 18:18:...| 8328.90|        5|    116|      2|      3|      5|        At Risk|\n",
      "|         14|2025-08-02 22:00:...| 6281.13|       11|     35|      4|      4|      5|      Champions|\n",
      "|         18|2025-08-12 18:18:...| 2885.38|        4|     25|      5|      2|      5|  New Customers|\n",
      "|        300|2024-10-28 03:00:...|   49.62|        1|    313|      1|      1|      1|           Lost|\n",
      "|         38|2025-09-03 20:43:...| 2478.07|        1|      3|      5|      1|      5|  New Customers|\n",
      "|        440|2025-09-01 09:35:...| 5905.81|       11|      5|      5|      4|      5|      Champions|\n",
      "|        677|2025-08-27 20:43:...| 7197.70|       16|     10|      5|      4|      5|      Champions|\n",
      "|        644|2025-06-06 18:47:...| 6979.64|        5|     92|      2|      3|      5|        At Risk|\n",
      "|        857|2025-06-10 20:20:...| 7968.51|       11|     88|      3|      4|      5|Loyal Customers|\n",
      "|        802|2025-08-25 00:19:...| 9939.54|        6|     12|      5|      3|      5|Loyal Customers|\n",
      "+-----------+--------------------+--------+---------+-------+-------+-------+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark RFM Analysis\n",
    "def calculate_rfm_analysis():\n",
    "    \"\"\"Calculate RFM scores with advanced segmentation using PySpark\"\"\"\n",
    "    \n",
    "    print(\"Calculating RFM Analysis...\")\n",
    "    \n",
    "    # Get analysis date\n",
    "    analysis_date = orders.select(max(\"order_date\")).collect()[0][0]\n",
    "    \n",
    "    # Calculate RFM base metrics\n",
    "    rfm_base = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(\n",
    "            max(\"order_date\").alias(\"last_order_date\"),\n",
    "            sum(\"total_amount\").alias(\"monetary\"),\n",
    "            count(\"order_id\").alias(\"frequency\")\n",
    "        )\n",
    "    \n",
    "    # Calculate recency\n",
    "    rfm_metrics = rfm_base.withColumn(\n",
    "        \"recency\", \n",
    "        datediff(lit(analysis_date), col(\"last_order_date\"))\n",
    "    )\n",
    "    \n",
    "    # Calculate RFM scores (1-5 scale)\n",
    "    rfm_scores = rfm_metrics.withColumn(\n",
    "        \"r_score\", \n",
    "        when(col(\"recency\") <= 30, 5)\n",
    "        .when(col(\"recency\") <= 60, 4)\n",
    "        .when(col(\"recency\") <= 90, 3)\n",
    "        .when(col(\"recency\") <= 180, 2)\n",
    "        .otherwise(1)\n",
    "    ).withColumn(\n",
    "        \"f_score\",\n",
    "        when(col(\"frequency\") >= 20, 5)\n",
    "        .when(col(\"frequency\") >= 10, 4)\n",
    "        .when(col(\"frequency\") >= 5, 3)\n",
    "        .when(col(\"frequency\") >= 2, 2)\n",
    "        .otherwise(1)\n",
    "    ).withColumn(\n",
    "        \"m_score\",\n",
    "        when(col(\"monetary\") >= 2000, 5)\n",
    "        .when(col(\"monetary\") >= 1000, 4)\n",
    "        .when(col(\"monetary\") >= 500, 3)\n",
    "        .when(col(\"monetary\") >= 100, 2)\n",
    "        .otherwise(1)\n",
    "    )\n",
    "    \n",
    "    # Create RFM segments\n",
    "    rfm_segments = rfm_scores.withColumn(\n",
    "        \"rfm_segment\",\n",
    "        when((col(\"r_score\") >= 4) & (col(\"f_score\") >= 4) & (col(\"m_score\") >= 4), \"Champions\")\n",
    "        .when((col(\"r_score\") >= 3) & (col(\"f_score\") >= 3) & (col(\"m_score\") >= 3), \"Loyal Customers\")\n",
    "        .when((col(\"r_score\") >= 4) & (col(\"f_score\") <= 2), \"New Customers\")\n",
    "        .when((col(\"r_score\") >= 3) & (col(\"f_score\") >= 2) & (col(\"m_score\") <= 2), \"Potential Loyalists\")\n",
    "        .when((col(\"r_score\") <= 2) & (col(\"f_score\") >= 3) & (col(\"m_score\") >= 3), \"At Risk\")\n",
    "        .when((col(\"r_score\") <= 2) & (col(\"f_score\") >= 2) & (col(\"m_score\") >= 2), \"Cannot Lose Them\")\n",
    "        .when((col(\"r_score\") <= 2) & (col(\"f_score\") <= 2) & (col(\"m_score\") <= 2), \"Lost\")\n",
    "        .otherwise(\"Others\")\n",
    "    )\n",
    "    \n",
    "    return rfm_segments\n",
    "\n",
    "# Execute RFM Analysis\n",
    "rfm_analysis = calculate_rfm_analysis()\n",
    "rfm_analysis.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce11c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Cohort Analysis...\n",
      "+-------------+-------------+---------+-------+\n",
      "|cohort_period|period_number|customers|revenue|\n",
      "+-------------+-------------+---------+-------+\n",
      "|      2022-09|          0.0|        2| 443.34|\n",
      "|      2022-09|          1.0|        2|2902.00|\n",
      "|      2022-09|          6.0|        1| 197.35|\n",
      "|      2022-09|          7.0|        1|  80.26|\n",
      "|      2022-09|          8.0|        1|2152.49|\n",
      "|      2022-09|         11.0|        1| 132.31|\n",
      "|      2022-09|         16.0|        1| 281.05|\n",
      "|      2022-09|         17.0|        1|1343.54|\n",
      "|      2022-09|         18.0|        1| 352.92|\n",
      "|      2022-09|         21.0|        1| 789.91|\n",
      "|      2022-09|         22.0|        1| 239.80|\n",
      "|      2022-09|         29.0|        1|5154.49|\n",
      "|      2022-09|         30.0|        1| 447.98|\n",
      "|      2022-09|         32.0|        2|3405.37|\n",
      "|      2022-09|         35.0|        1|1937.24|\n",
      "|      2022-09|         36.0|        1|2398.61|\n",
      "|      2022-10|          0.0|        3|1302.80|\n",
      "|      2022-10|          1.0|        1| 563.36|\n",
      "|      2022-10|          3.0|        1| 302.98|\n",
      "|      2022-10|          4.0|        1| 541.06|\n",
      "+-------------+-------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark Cohort Analysis\n",
    "def cohort_analysis():\n",
    "    \"\"\"Perform cohort analysis using PySpark\"\"\"\n",
    "    \n",
    "    print(\"Performing Cohort Analysis...\")\n",
    "    \n",
    "    # Get first order date for each customer\n",
    "    customer_cohorts = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(min(\"order_date\").alias(\"first_order_date\"))\n",
    "    \n",
    "    # Create cohort orders with first order date\n",
    "    cohort_orders = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .join(customer_cohorts, \"customer_id\") \\\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"order_date\",\n",
    "            \"first_order_date\",\n",
    "            \"total_amount\"\n",
    "        )\n",
    "    \n",
    "    # Calculate cohort period and order period\n",
    "    cohort_data = cohort_orders.withColumn(\n",
    "        \"cohort_period\", \n",
    "        date_format(col(\"first_order_date\"), \"yyyy-MM\")\n",
    "    ).withColumn(\n",
    "        \"order_period\",\n",
    "        date_format(col(\"order_date\"), \"yyyy-MM\")\n",
    "    ).withColumn(\n",
    "        \"period_number\",\n",
    "        months_between(\n",
    "            to_date(col(\"order_period\"), \"yyyy-MM\"),\n",
    "            to_date(col(\"cohort_period\"), \"yyyy-MM\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate cohort metrics\n",
    "    cohort_metrics = cohort_data.groupBy(\"cohort_period\", \"period_number\") \\\n",
    "        .agg(\n",
    "            countDistinct(\"customer_id\").alias(\"customers\"),\n",
    "            sum(\"total_amount\").alias(\"revenue\")\n",
    "        ) \\\n",
    "        .orderBy(\"cohort_period\", \"period_number\")\n",
    "    \n",
    "    return cohort_metrics\n",
    "\n",
    "# Execute Cohort Analysis\n",
    "cohort_data = cohort_analysis()\n",
    "cohort_data.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2894a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Customer Features...\n",
      "Performing ML Customer Clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/06 18:57:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: 5\n",
      "✅ Customer Analytics Complete\n",
      "+-----------+-------+------------+-----------+\n",
      "|customer_id|cluster|total_orders|total_spent|\n",
      "+-----------+-------+------------+-----------+\n",
      "|        881|      4|           5|    8328.90|\n",
      "|         14|      2|          11|    6281.13|\n",
      "|         18|      1|           4|    2885.38|\n",
      "|        300|      0|           1|      49.62|\n",
      "|         38|      4|           1|    2478.07|\n",
      "|        440|      2|          11|    5905.81|\n",
      "|        677|      2|          16|    7197.70|\n",
      "|        644|      4|           5|    6979.64|\n",
      "|        857|      2|          11|    7968.51|\n",
      "|        802|      4|           6|    9939.54|\n",
      "+-----------+-------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark Customer Features and ML Clustering\n",
    "def create_customer_features():\n",
    "    \"\"\"Create comprehensive customer features using PySpark\"\"\"\n",
    "    \n",
    "    print(\"Creating Customer Features...\")\n",
    "    \n",
    "    # Customer order patterns\n",
    "    customer_patterns = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"total_amount\").alias(\"total_spent\"),\n",
    "            avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "            max(\"order_date\").alias(\"last_order_date\"),\n",
    "            min(\"order_date\").alias(\"first_order_date\"),\n",
    "            countDistinct(\"order_date\").alias(\"unique_order_days\"),\n",
    "            stddev(\"total_amount\").alias(\"order_value_std\")\n",
    "        )\n",
    "    \n",
    "    # Calculate additional features\n",
    "    customer_features = customer_patterns.withColumn(\n",
    "        \"days_since_last_order\",\n",
    "        datediff(current_date(), col(\"last_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"customer_lifespan_days\",\n",
    "        datediff(col(\"last_order_date\"), col(\"first_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"order_frequency\",\n",
    "        when(col(\"customer_lifespan_days\") > 0, \n",
    "             col(\"total_orders\") / col(\"customer_lifespan_days\") * 30)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Join with customer demographics\n",
    "    customer_demographics = customers.select(\n",
    "        \"customer_id\", \"country\", \"state\", \"city\", \"segment\"\n",
    "    )\n",
    "    \n",
    "    final_features = customer_features.join(customer_demographics, \"customer_id\", \"left\")\n",
    "    \n",
    "    return final_features\n",
    "\n",
    "def ml_customer_clustering(customer_features, rfm_analysis):\n",
    "    \"\"\"Perform ML clustering using PySpark MLlib\"\"\"\n",
    "    \n",
    "    print(\"Performing ML Customer Clustering...\")\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    feature_columns = [\n",
    "        \"total_orders\", \"total_spent\", \"avg_order_value\", \n",
    "        \"days_since_last_order\", \"order_frequency\", \"order_value_std\"\n",
    "    ]\n",
    "    \n",
    "    # Handle null values\n",
    "    clustering_data = customer_features.fillna(0, subset=feature_columns)\n",
    "    \n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    feature_vector = assembler.transform(clustering_data)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    scaler_model = scaler.fit(feature_vector)\n",
    "    scaled_data = scaler_model.transform(feature_vector)\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        predictionCol=\"cluster\",\n",
    "        k=5,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    model = kmeans.fit(scaled_data)\n",
    "    clustered_data = model.transform(scaled_data)\n",
    "    \n",
    "    # Add cluster centers\n",
    "    cluster_centers = model.clusterCenters()\n",
    "    print(f\"Cluster Centers: {len(cluster_centers)}\")\n",
    "    \n",
    "    return clustered_data\n",
    "\n",
    "# Execute Customer Features and Clustering\n",
    "customer_features = create_customer_features()\n",
    "customer_clusters = ml_customer_clustering(customer_features, rfm_analysis)\n",
    "\n",
    "print(\"✅ Customer Analytics Complete\")\n",
    "customer_clusters.select(\"customer_id\", \"cluster\", \"total_orders\", \"total_spent\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Analytics Results...\n",
      "✅ Saved rfm_analysis: 922 records\n",
      "✅ Saved cohort_analysis: 651 records\n",
      "✅ Saved customer_features: 922 records\n",
      "❌ Error saving customer_clusters: Can't get JDBC type for struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.\n",
      "✅ All analytics saved to database\n"
     ]
    }
   ],
   "source": [
    "# PySpark Data Saving Functions\n",
    "def save_to_database_spark(df, table_name):\n",
    "    \"\"\"Save Spark DataFrame to SQL Server using JDBC\"\"\"\n",
    "    \n",
    "    jdbc_url = jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=ecom_db;encrypt=true;trustServerCertificate=true\"\n",
    "    connection_properties = {\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Gova#ss123\",\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Write to database\n",
    "        df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .jdbc(jdbc_url, table_name, properties=connection_properties)\n",
    "        \n",
    "        print(f\"✅ Saved {table_name}: {df.count():,} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving {table_name}: {e}\")\n",
    "\n",
    "# Save all analytics results\n",
    "print(\"Saving Analytics Results...\")\n",
    "save_to_database_spark(rfm_analysis, \"rfm_analysis\")\n",
    "save_to_database_spark(cohort_data, \"cohort_analysis\") \n",
    "save_to_database_spark(customer_features, \"customer_features\")  \n",
    "save_to_database_spark(customer_clusters, \"customer_clusters\")\n",
    "\n",
    "print(\"✅ All analytics saved to database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49dbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🚀 Starting Advanced Customer Analytics with PySpark\n",
      "================================================================================\n",
      "\n",
      "📊 Calculating RFM Analysis...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'orders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 206\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrfm_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m: rfm_analysis,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohort_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m: cohort_analysis,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: customer_features_final,\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_clusters\u001b[39m\u001b[38;5;124m\"\u001b[39m: customer_clusters\n\u001b[1;32m    203\u001b[0m     }\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Execute the complete pipeline\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43madvanced_customer_analytics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36madvanced_customer_analytics\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. RFM Analysis\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Calculating RFM Analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m analysis_date \u001b[38;5;241m=\u001b[39m \u001b[43morders\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m rfm_base \u001b[38;5;241m=\u001b[39m orders\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     21\u001b[0m rfm_metrics \u001b[38;5;241m=\u001b[39m rfm_base\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     23\u001b[0m     datediff(lit(analysis_date), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_order_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orders' is not defined"
     ]
    }
   ],
   "source": [
    "# Complete PySpark ETL Pipeline - Execute All Analytics\n",
    "def advanced_customer_analytics():\n",
    "    \"\"\"Complete customer analytics pipeline using PySpark\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"🚀 Starting Advanced Customer Analytics with PySpark\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. RFM Analysis\n",
    "    print(\"\\n📊 Calculating RFM Analysis...\")\n",
    "    analysis_date = orders.select(max(\"order_date\")).collect()[0][0]\n",
    "    \n",
    "    rfm_base = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(\n",
    "            max(\"order_date\").alias(\"last_order_date\"),\n",
    "            sum(\"total_amount\").alias(\"monetary\"),\n",
    "            count(\"order_id\").alias(\"frequency\")\n",
    "        )\n",
    "    \n",
    "    rfm_metrics = rfm_base.withColumn(\n",
    "        \"recency\", \n",
    "        datediff(lit(analysis_date), col(\"last_order_date\"))\n",
    "    )\n",
    "    \n",
    "    rfm_scores = rfm_metrics.withColumn(\n",
    "        \"r_score\", \n",
    "        when(col(\"recency\") <= 30, 5)\n",
    "        .when(col(\"recency\") <= 60, 4)\n",
    "        .when(col(\"recency\") <= 90, 3)\n",
    "        .when(col(\"recency\") <= 180, 2)\n",
    "        .otherwise(1)\n",
    "    ).withColumn(\n",
    "        \"f_score\",\n",
    "        when(col(\"frequency\") >= 20, 5)\n",
    "        .when(col(\"frequency\") >= 10, 4)\n",
    "        .when(col(\"frequency\") >= 5, 3)\n",
    "        .when(col(\"frequency\") >= 2, 2)\n",
    "        .otherwise(1)\n",
    "    ).withColumn(\n",
    "        \"m_score\",\n",
    "        when(col(\"monetary\") >= 2000, 5)\n",
    "        .when(col(\"monetary\") >= 1000, 4)\n",
    "        .when(col(\"monetary\") >= 500, 3)\n",
    "        .when(col(\"monetary\") >= 100, 2)\n",
    "        .otherwise(1)\n",
    "    )\n",
    "    \n",
    "    rfm_analysis = rfm_scores.withColumn(\n",
    "        \"rfm_segment\",\n",
    "        when((col(\"r_score\") >= 4) & (col(\"f_score\") >= 4) & (col(\"m_score\") >= 4), \"Champions\")\n",
    "        .when((col(\"r_score\") >= 3) & (col(\"f_score\") >= 3) & (col(\"m_score\") >= 3), \"Loyal Customers\")\n",
    "        .when((col(\"r_score\") >= 4) & (col(\"f_score\") <= 2), \"New Customers\")\n",
    "        .when((col(\"r_score\") >= 3) & (col(\"f_score\") >= 2) & (col(\"m_score\") <= 2), \"Potential Loyalists\")\n",
    "        .when((col(\"r_score\") <= 2) & (col(\"f_score\") >= 3) & (col(\"m_score\") >= 3), \"At Risk\")\n",
    "        .when((col(\"r_score\") <= 2) & (col(\"f_score\") >= 2) & (col(\"m_score\") >= 2), \"Cannot Lose Them\")\n",
    "        .when((col(\"r_score\") <= 2) & (col(\"f_score\") <= 2) & (col(\"m_score\") <= 2), \"Lost\")\n",
    "        .otherwise(\"Others\")\n",
    "    )\n",
    "    \n",
    "    print(\"✅ RFM Analysis Complete\")\n",
    "    rfm_analysis.show(5)\n",
    "    \n",
    "    # 2. Cohort Analysis\n",
    "    print(\"\\n📈 Performing Cohort Analysis...\")\n",
    "    customer_cohorts = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(min(\"order_date\").alias(\"first_order_date\"))\n",
    "    \n",
    "    cohort_orders = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .join(customer_cohorts, \"customer_id\") \\\n",
    "        .select(\"customer_id\", \"order_date\", \"first_order_date\", \"total_amount\")\n",
    "    \n",
    "    cohort_data = cohort_orders.withColumn(\n",
    "        \"cohort_period\", \n",
    "        date_format(col(\"first_order_date\"), \"yyyy-MM\")\n",
    "    ).withColumn(\n",
    "        \"order_period\",\n",
    "        date_format(col(\"order_date\"), \"yyyy-MM\")\n",
    "    ).withColumn(\n",
    "        \"period_number\",\n",
    "        months_between(\n",
    "            to_date(col(\"order_period\"), \"yyyy-MM\"),\n",
    "            to_date(col(\"cohort_period\"), \"yyyy-MM\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    cohort_analysis = cohort_data.groupBy(\"cohort_period\", \"period_number\") \\\n",
    "        .agg(\n",
    "            countDistinct(\"customer_id\").alias(\"customers\"),\n",
    "            sum(\"total_amount\").alias(\"revenue\")\n",
    "        ) \\\n",
    "        .orderBy(\"cohort_period\", \"period_number\")\n",
    "    \n",
    "    print(\"✅ Cohort Analysis Complete\")\n",
    "    cohort_analysis.show(5)\n",
    "    \n",
    "    # 3. Customer Features\n",
    "    print(\"\\n🔍 Creating Customer Features...\")\n",
    "    customer_patterns = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"total_amount\").alias(\"total_spent\"),\n",
    "            avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "            max(\"order_date\").alias(\"last_order_date\"),\n",
    "            min(\"order_date\").alias(\"first_order_date\"),\n",
    "            countDistinct(\"order_date\").alias(\"unique_order_days\"),\n",
    "            stddev(\"total_amount\").alias(\"order_value_std\")\n",
    "        )\n",
    "    \n",
    "    customer_features = customer_patterns.withColumn(\n",
    "        \"days_since_last_order\",\n",
    "        datediff(current_date(), col(\"last_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"customer_lifespan_days\",\n",
    "        datediff(col(\"last_order_date\"), col(\"first_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"order_frequency\",\n",
    "        when(col(\"customer_lifespan_days\") > 0, \n",
    "             col(\"total_orders\") / col(\"customer_lifespan_days\") * 30)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    customer_demographics = customers.select(\n",
    "        \"customer_id\", \"country\", \"state\", \"city\", \"segment\"\n",
    "    )\n",
    "    \n",
    "    customer_features_final = customer_features.join(customer_demographics, \"customer_id\", \"left\")\n",
    "    \n",
    "    print(\"✅ Customer Features Complete\")\n",
    "    customer_features_final.show(5)\n",
    "    \n",
    "    # 4. ML Clustering\n",
    "    print(\"\\n🤖 Performing ML Clustering...\")\n",
    "    feature_columns = [\n",
    "        \"total_orders\", \"total_spent\", \"avg_order_value\", \n",
    "        \"days_since_last_order\", \"order_frequency\", \"order_value_std\"\n",
    "    ]\n",
    "    \n",
    "    clustering_data = customer_features_final.fillna(0, subset=feature_columns)\n",
    "    \n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    feature_vector = assembler.transform(clustering_data)\n",
    "    \n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    scaler_model = scaler.fit(feature_vector)\n",
    "    scaled_data = scaler_model.transform(feature_vector)\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        predictionCol=\"cluster\",\n",
    "        k=5,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    model = kmeans.fit(scaled_data)\n",
    "    customer_clusters = model.transform(scaled_data)\n",
    "    \n",
    "    print(\"✅ ML Clustering Complete\")\n",
    "    customer_clusters.select(\"customer_id\", \"cluster\", \"total_orders\", \"total_spent\").show(5)\n",
    "    \n",
    "    # 5. Save to Database\n",
    "    print(\"\\n💾 Saving Results to Database...\")\n",
    "    \n",
    "    def save_to_database_spark(df, table_name):\n",
    "        jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=ecom_analytics\"\n",
    "        connection_properties = {\n",
    "            \"user\": \"sa\",\n",
    "            \"password\": \"Gova#ss123\",\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            df.write.mode(\"overwrite\").jdbc(jdbc_url, table_name, properties=connection_properties)\n",
    "            print(f\"✅ Saved {table_name}: {df.count():,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving {table_name}: {e}\")\n",
    "    \n",
    "    save_to_database_spark(rfm_analysis, \"rfm_analysis\")\n",
    "    save_to_database_spark(cohort_analysis, \"cohort_analysis\") \n",
    "    save_to_database_spark(customer_features_final, \"customer_features\")\n",
    "    save_to_database_spark(customer_clusters, \"customer_clusters\")\n",
    "    \n",
    "    print(\"\\n🎉 Customer Analytics Pipeline Complete!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"rfm_analysis\": rfm_analysis,\n",
    "        \"cohort_analysis\": cohort_analysis,\n",
    "        \"customer_features\": customer_features_final,\n",
    "        \"customer_clusters\": customer_clusters\n",
    "    }\n",
    "\n",
    "# Execute the complete pipeline\n",
    "results = advanced_customer_analytics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f20bf0",
   "metadata": {},
   "source": [
    "# SECTION 1: DATABASE SCHEMA AND TABLE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17139c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFIED ETL: Enhance existing tables instead of creating many new ones\n",
    "print(\"=\"*60)\n",
    "print(\"🚀 SIMPLIFIED ETL: Enhancing existing tables with analytics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Enhance customers table with RFM and clustering data\n",
    "print(\"\\n📊 Adding RFM and clustering to customers table...\")\n",
    "customers_enhanced = tables[\"customers\"].join(\n",
    "    rfm_analysis.select(\"customer_id\", \"recency_score\", \"frequency_score\", \"monetary_score\", \"rfm_segment\"),\n",
    "    \"customer_id\", \"left\"\n",
    ").join(\n",
    "    customer_clusters.select(\"customer_id\", \"cluster\", \"cluster_name\"),\n",
    "    \"customer_id\", \"left\"\n",
    ")\n",
    "\n",
    "# 2. Enhance orders table with customer features\n",
    "print(\"📊 Adding customer features to orders table...\")\n",
    "orders_enhanced = tables[\"orders\"].join(\n",
    "    customer_features.select(\"customer_id\", \"total_orders\", \"total_spent\", \"avg_order_value\", \"days_since_last_order\"),\n",
    "    \"customer_id\", \"left\"\n",
    ")\n",
    "\n",
    "# 3. Function to save enhanced tables\n",
    "def save_enhanced_table(df, table_name):\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", connection_properties[\"user\"]) \\\n",
    "            .option(\"password\", connection_properties[\"password\"]) \\\n",
    "            .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        print(f\"✅ Enhanced {table_name}: {df.count():,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error enhancing {table_name}: {e}\")\n",
    "\n",
    "# 4. Save enhanced tables back to original table names\n",
    "print(\"\\n💾 Saving enhanced tables...\")\n",
    "save_enhanced_table(customers_enhanced, \"customers\")\n",
    "save_enhanced_table(orders_enhanced, \"orders\")\n",
    "\n",
    "print(\"\\n🎉 ETL Complete! Tables enhanced with analytics data\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analytics_database_and_schema():\n",
    "    \"\"\"Create analytics database and schema using ODBC\"\"\"\n",
    "    \n",
    "    print(\"Creating analytics database and schema...\")\n",
    "    \n",
    "    try:\n",
    "        # --------------------------\n",
    "        # Step 1: Create database\n",
    "        # --------------------------\n",
    "        master_conn = get_odbc_connection(\n",
    "            ODBC_CONFIG[\"target_db\"], \n",
    "            database_override=\"master\", \n",
    "            autocommit=True  # <--- important for CREATE DATABASE\n",
    "        )\n",
    "        cursor = master_conn.cursor()\n",
    "        create_db_sql = \"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = 'ecom_analytics')\n",
    "        BEGIN\n",
    "            CREATE DATABASE ecom_analytics;\n",
    "        END\n",
    "        \"\"\"\n",
    "        cursor.execute(create_db_sql)\n",
    "        cursor.close()\n",
    "        master_conn.close()\n",
    "        print(\"✅ Analytics database created/verified\")\n",
    "        \n",
    "        # --------------------------\n",
    "        # Step 2: Create schema\n",
    "        # --------------------------\n",
    "        analytics_conn = get_odbc_connection(\n",
    "            ODBC_CONFIG[\"target_db\"], \n",
    "            autocommit=False  # schema can run in a transaction\n",
    "        )\n",
    "        cursor = analytics_conn.cursor()\n",
    "        create_schema_sql = \"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'analytics')\n",
    "        BEGIN\n",
    "            EXEC('CREATE SCHEMA analytics');\n",
    "        END\n",
    "        \"\"\"\n",
    "        cursor.execute(create_schema_sql)\n",
    "        analytics_conn.commit()\n",
    "        cursor.close()\n",
    "        analytics_conn.close()\n",
    "        print(\"✅ Analytics schema created/verified\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating database/schema: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de80dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_if_not_exists(table_name, create_sql):\n",
    "    \"\"\"Helper function to create table if it doesn't exist using ODBC\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # DDL safe with autocommit\n",
    "        conn = get_odbc_connection(ODBC_CONFIG[\"target_db\"], autocommit=True)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check if table exists\n",
    "        check_sql = \"\"\"\n",
    "        SELECT COUNT(*) FROM sys.tables t \n",
    "        JOIN sys.schemas s ON t.schema_id = s.schema_id \n",
    "        WHERE s.name = 'analytics' AND t.name = ?\n",
    "        \"\"\"\n",
    "        cursor.execute(check_sql, table_name)\n",
    "        exists = cursor.fetchone()[0] > 0\n",
    "        \n",
    "        if not exists:\n",
    "            cursor.execute(create_sql)  # Must be fully formatted string\n",
    "            print(f\"✅ Created table analytics.{table_name}\")\n",
    "        else:\n",
    "            print(f\"✅ Table analytics.{table_name} already exists\")\n",
    "            \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating table {table_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47892345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_database_spark(df, table_name, mode=\"overwrite\"):\n",
    "    \"\"\"Save DataFrame to database table using Spark JDBC\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df_optimized = df.coalesce(10) if df.rdd.getNumPartitions() > 10 else df\n",
    "        \n",
    "        df_optimized.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", TARGET_URL) \\\n",
    "            .option(\"dbtable\", f\"analytics.{table_name}\") \\\n",
    "            .option(\"user\", TARGET_ODBC_CONN[\"user\"]) \\\n",
    "            .option(\"password\", TARGET_ODBC_CONN[\"password\"]) \\\n",
    "            .option(\"driver\", TARGET_ODBC_CONN[\"driver\"]) \\\n",
    "            .option(\"batchsize\", \"5000\") \\\n",
    "            .option(\"isolationLevel\", \"READ_UNCOMMITTED\") \\\n",
    "            .mode(mode) \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"✅ Saved {df.count():,} records to analytics.{table_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving to {table_name}: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a75564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_database_spark_jdbc(table_name, columns=\"*\", predicates=None):\n",
    "    \"\"\"Read DataFrame from database table using Spark JDBC\"\"\"\n",
    "    \n",
    "    try:\n",
    "        dbtable = f\"(SELECT {columns} FROM analytics.{table_name}) AS temp\" \\\n",
    "                  if columns != \"*\" else f\"analytics.{table_name}\"\n",
    "        \n",
    "        df_reader = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", TARGET_URL) \\\n",
    "            .option(\"dbtable\", dbtable) \\\n",
    "            .option(\"user\", TARGET_ODBC_CONN[\"user\"]) \\\n",
    "            .option(\"password\", TARGET_ODBC_CONN[\"password\"]) \\\n",
    "            .option(\"driver\", TARGET_ODBC_CONN[\"driver\"])\n",
    "        \n",
    "        if predicates:\n",
    "            df_reader = df_reader.option(\"predicate\", predicates)\n",
    "        \n",
    "        df = df_reader.load()\n",
    "        print(f\"✅ Read {df.count():,} records from analytics.{table_name}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading from {table_name}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analytics database and schema...\n",
      "✅ Analytics database created/verified\n",
      "✅ Analytics schema created/verified\n"
     ]
    }
   ],
   "source": [
    "create_analytics_database_and_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa3d95",
   "metadata": {},
   "source": [
    "# SECTION 2: CREATE ALL ANALYTICS TABLES WITH ODBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_analytics_tables():\n",
    "    \"\"\"Create all analytics tables with proper schemas using ODBC\"\"\"\n",
    "    \n",
    "    print(\"Creating analytics tables...\")\n",
    "    \n",
    "    # Customer Analytics Tables\n",
    "    customer_features_sql = \"\"\"\n",
    "    CREATE TABLE analytics.customer_features (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        first_name NVARCHAR(60),\n",
    "        last_name NVARCHAR(60),\n",
    "        email NVARCHAR(255),\n",
    "        country NVARCHAR(80),\n",
    "        state NVARCHAR(80),\n",
    "        city NVARCHAR(80),\n",
    "        segment NVARCHAR(30),\n",
    "        customer_tier NVARCHAR(20),\n",
    "        total_orders INT,\n",
    "        total_spent DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        customer_age_days INT,\n",
    "        days_since_last_order INT,\n",
    "        order_frequency DECIMAL(10,6),\n",
    "        payment_method_diversity INT,\n",
    "        signup_to_first_order_days INT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    rfm_analysis_sql = \"\"\"\n",
    "    CREATE TABLE analytics.rfm_analysis (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        last_order_date DATETIME2,\n",
    "        frequency INT,\n",
    "        monetary_value DECIMAL(14,2),\n",
    "        first_order_date DATETIME2,\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        recency INT,\n",
    "        customer_lifespan INT,\n",
    "        r_score INT,\n",
    "        f_score INT,\n",
    "        m_score INT,\n",
    "        rfm_segment NVARCHAR(30),\n",
    "        rfm_score INT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    cohort_analysis_sql = \"\"\"\n",
    "    CREATE TABLE analytics.cohort_analysis (\n",
    "        cohort_month DATE,\n",
    "        months_since_first_purchase INT,\n",
    "        customers INT,\n",
    "        orders INT,\n",
    "        revenue DECIMAL(14,2),\n",
    "        cohort_size INT,\n",
    "        retention_rate DECIMAL(5,4),\n",
    "        revenue_per_customer DECIMAL(10,2),\n",
    "        created_date DATETIME2 DEFAULT GETDATE(),\n",
    "        PRIMARY KEY (cohort_month, months_since_first_purchase)\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    customer_clusters_sql = \"\"\"\n",
    "    CREATE TABLE analytics.customer_clusters (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        cluster INT,\n",
    "        cluster_name NVARCHAR(50),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    geographic_analysis_sql = \"\"\"\n",
    "    CREATE TABLE analytics.geographic_analysis (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        country NVARCHAR(80),\n",
    "        state NVARCHAR(80),\n",
    "        city NVARCHAR(80),\n",
    "        customer_count INT,\n",
    "        avg_customer_value DECIMAL(10,2),\n",
    "        total_market_value DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        avg_orders_per_customer DECIMAL(8,2),\n",
    "        market_penetration_score DECIMAL(14,2),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Product Analytics Tables\n",
    "    product_performance_sql = \"\"\"\n",
    "    CREATE TABLE analytics.product_performance (\n",
    "        product_id INT PRIMARY KEY,\n",
    "        sku NVARCHAR(64),\n",
    "        product_name NVARCHAR(255),\n",
    "        category NVARCHAR(80),\n",
    "        brand NVARCHAR(80),\n",
    "        price DECIMAL(10,2),\n",
    "        total_quantity_sold INT,\n",
    "        total_revenue DECIMAL(14,2),\n",
    "        total_line_items INT,\n",
    "        unique_orders INT,\n",
    "        unique_customers INT,\n",
    "        avg_selling_price DECIMAL(10,2),\n",
    "        min_selling_price DECIMAL(10,2),\n",
    "        max_selling_price DECIMAL(10,2),\n",
    "        price_volatility DECIMAL(10,2),\n",
    "        return_rate DECIMAL(5,4),\n",
    "        cancellation_rate DECIMAL(5,4),\n",
    "        stock_level INT,\n",
    "        reorder_threshold INT,\n",
    "        revenue_per_unit DECIMAL(10,2),\n",
    "        inventory_turnover DECIMAL(8,4),\n",
    "        stockout_risk NVARCHAR(10),\n",
    "        profit_margin DECIMAL(5,4),\n",
    "        lifecycle_stage NVARCHAR(20),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    market_basket_analysis_sql = \"\"\"\n",
    "    CREATE TABLE analytics.market_basket_analysis (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        product_a INT,\n",
    "        product_b INT,\n",
    "        co_occurrence_count INT,\n",
    "        support DECIMAL(8,6),\n",
    "        confidence_a_to_b DECIMAL(5,4),\n",
    "        confidence_b_to_a DECIMAL(5,4),\n",
    "        lift DECIMAL(8,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    category_performance_sql = \"\"\"\n",
    "    CREATE TABLE analytics.category_performance (\n",
    "        category NVARCHAR(80) PRIMARY KEY,\n",
    "        total_products INT,\n",
    "        category_quantity_sold BIGINT,\n",
    "        category_revenue DECIMAL(14,2),\n",
    "        avg_category_price DECIMAL(10,2),\n",
    "        avg_return_rate DECIMAL(5,4),\n",
    "        avg_inventory_turnover DECIMAL(8,4),\n",
    "        brand_diversity INT,\n",
    "        market_share DECIMAL(5,4),\n",
    "        revenue_per_product DECIMAL(12,2),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sales Analytics Tables\n",
    "    sales_timeseries_sql = \"\"\"\n",
    "    CREATE TABLE analytics.sales_timeseries (\n",
    "        order_date DATE PRIMARY KEY,\n",
    "        daily_orders INT,\n",
    "        daily_revenue DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        unique_customers INT,\n",
    "        total_shipping DECIMAL(10,2),\n",
    "        total_tax DECIMAL(10,2),\n",
    "        total_discounts DECIMAL(10,2),\n",
    "        day_of_week INT,\n",
    "        day_name NVARCHAR(10),\n",
    "        month INT,\n",
    "        quarter INT,\n",
    "        year INT,\n",
    "        is_weekend BIT,\n",
    "        revenue_7d_ma DECIMAL(14,2),\n",
    "        revenue_30d_ma DECIMAL(14,2),\n",
    "        revenue_growth DECIMAL(8,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_analysis_sql = \"\"\"\n",
    "    CREATE TABLE analytics.channel_analysis (\n",
    "        placed_via NVARCHAR(64) PRIMARY KEY,\n",
    "        total_orders INT,\n",
    "        total_revenue DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        unique_customers INT,\n",
    "        total_discounts DECIMAL(10,2),\n",
    "        revenue_per_customer DECIMAL(10,2),\n",
    "        discount_rate DECIMAL(5,4),\n",
    "        market_share DECIMAL(5,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    conversion_funnel_sql = \"\"\"\n",
    "    CREATE TABLE analytics.conversion_funnel (\n",
    "        signup_month DATE PRIMARY KEY,\n",
    "        signups INT,\n",
    "        conversions INT,\n",
    "        avg_days_to_convert DECIMAL(8,2),\n",
    "        conversion_rate DECIMAL(5,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    customer_acquisition_sql = \"\"\"\n",
    "    CREATE TABLE analytics.customer_acquisition (\n",
    "        acquisition_month DATE PRIMARY KEY,\n",
    "        new_customers INT,\n",
    "        new_customer_revenue DECIMAL(14,2),\n",
    "        avg_first_order_value DECIMAL(10,2),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clickstream Analytics Tables\n",
    "    session_metrics_sql = \"\"\"\n",
    "    CREATE TABLE analytics.session_metrics (\n",
    "        session_id NVARCHAR(64) PRIMARY KEY,\n",
    "        customer_id INT,\n",
    "        device NVARCHAR(40),\n",
    "        page_views INT,\n",
    "        unique_pages INT,\n",
    "        products_viewed INT,\n",
    "        session_start DATETIME2,\n",
    "        session_end DATETIME2,\n",
    "        session_duration_minutes DECIMAL(8,2),\n",
    "        bounce BIT,\n",
    "        traffic_source NVARCHAR(80),\n",
    "        referrer NVARCHAR(255),\n",
    "        completed_purchase BIT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    user_journey_flow_sql = \"\"\"\n",
    "    CREATE TABLE analytics.user_journey_flow (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        page NVARCHAR(80),\n",
    "        next_page NVARCHAR(80),\n",
    "        transition_count INT,\n",
    "        total_from_page INT,\n",
    "        transition_probability DECIMAL(5,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    user_behavior_segments_sql = \"\"\"\n",
    "    CREATE TABLE analytics.user_behavior_segments (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        total_clicks INT,\n",
    "        total_sessions INT,\n",
    "        unique_pages_visited INT,\n",
    "        unique_products_viewed INT,\n",
    "        devices_used INT,\n",
    "        clicks_per_session DECIMAL(8,2),\n",
    "        product_focus DECIMAL(5,4),\n",
    "        source_diversity INT,\n",
    "        engagement_level NVARCHAR(20),\n",
    "        session_intensity NVARCHAR(20),\n",
    "        shopping_focus NVARCHAR(30),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # ML Features Tables\n",
    "    clv_features_sql = \"\"\"\n",
    "    CREATE TABLE analytics.clv_features (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        total_orders INT,\n",
    "        total_spent DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        customer_age_days INT,\n",
    "        days_since_last_order INT,\n",
    "        order_frequency DECIMAL(10,6),\n",
    "        r_score INT,\n",
    "        f_score INT,\n",
    "        m_score INT,\n",
    "        high_value_customer BIT,\n",
    "        orders_per_day DECIMAL(10,6),\n",
    "        revenue_per_day DECIMAL(10,2),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    churn_features_sql = \"\"\"\n",
    "    CREATE TABLE analytics.churn_features (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        last_order_date DATETIME2,\n",
    "        recent_orders INT,\n",
    "        avg_recent_order_value DECIMAL(10,2),\n",
    "        days_since_last_order INT,\n",
    "        churn_risk BIT,\n",
    "        total_orders INT,\n",
    "        total_spent DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        r_score INT,\n",
    "        f_score INT,\n",
    "        m_score INT,\n",
    "        churn_risk_score DECIMAL(5,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dashboard Tables\n",
    "    executive_summary_sql = \"\"\"\n",
    "    CREATE TABLE analytics.executive_summary (\n",
    "        metric_name NVARCHAR(50) PRIMARY KEY,\n",
    "        metric_value DECIMAL(18,2),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    top_customers_sql = \"\"\"\n",
    "    CREATE TABLE analytics.top_customers (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        rank_position INT,\n",
    "        customer_id INT,\n",
    "        first_name NVARCHAR(60),\n",
    "        last_name NVARCHAR(60),\n",
    "        total_spent DECIMAL(14,2),\n",
    "        total_orders INT,\n",
    "        customer_tier NVARCHAR(20),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    top_products_sql = \"\"\"\n",
    "    CREATE TABLE analytics.top_products (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        rank_position INT,\n",
    "        product_id INT,\n",
    "        product_name NVARCHAR(255),\n",
    "        category NVARCHAR(80),\n",
    "        total_revenue DECIMAL(14,2),\n",
    "        total_quantity_sold INT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data Warehouse Tables\n",
    "    dim_customer_sql = \"\"\"\n",
    "    CREATE TABLE analytics.dim_customer (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        first_name NVARCHAR(60),\n",
    "        last_name NVARCHAR(60),\n",
    "        email NVARCHAR(255),\n",
    "        country NVARCHAR(80),\n",
    "        state NVARCHAR(80),\n",
    "        city NVARCHAR(80),\n",
    "        segment NVARCHAR(30),\n",
    "        customer_tier NVARCHAR(20),\n",
    "        rfm_segment NVARCHAR(30),\n",
    "        cluster_id INT,\n",
    "        cluster_name NVARCHAR(50),\n",
    "        total_orders INT,\n",
    "        total_spent DECIMAL(14,2),\n",
    "        effective_date DATE,\n",
    "        is_current BIT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    dim_product_sql = \"\"\"\n",
    "    CREATE TABLE analytics.dim_product (\n",
    "        product_id INT PRIMARY KEY,\n",
    "        sku NVARCHAR(64),\n",
    "        product_name NVARCHAR(255),\n",
    "        category NVARCHAR(80),\n",
    "        brand NVARCHAR(80),\n",
    "        price DECIMAL(10,2),\n",
    "        lifecycle_stage NVARCHAR(20),\n",
    "        stockout_risk NVARCHAR(10),\n",
    "        profit_margin DECIMAL(5,4),\n",
    "        total_quantity_sold INT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    dim_date_sql = \"\"\"\n",
    "    CREATE TABLE analytics.dim_date (\n",
    "        date DATE PRIMARY KEY,\n",
    "        year INT,\n",
    "        month INT,\n",
    "        day INT,\n",
    "        quarter INT,\n",
    "        day_of_week INT,\n",
    "        day_name NVARCHAR(10),\n",
    "        month_name NVARCHAR(10),\n",
    "        is_weekend BIT,\n",
    "        is_month_end BIT,\n",
    "        fiscal_year INT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    fact_sales_sql = \"\"\"\n",
    "    CREATE TABLE analytics.fact_sales (\n",
    "        date DATE PRIMARY KEY,\n",
    "        daily_orders INT,\n",
    "        daily_revenue DECIMAL(14,2),\n",
    "        avg_order_value DECIMAL(10,2),\n",
    "        unique_customers INT,\n",
    "        total_shipping DECIMAL(10,2),\n",
    "        total_tax DECIMAL(10,2),\n",
    "        total_discounts DECIMAL(10,2),\n",
    "        revenue_7d_ma DECIMAL(14,2),\n",
    "        revenue_30d_ma DECIMAL(14,2),\n",
    "        revenue_growth DECIMAL(8,4),\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    fact_customer_behavior_sql = \"\"\"\n",
    "    CREATE TABLE analytics.fact_customer_behavior (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        customer_id INT,\n",
    "        date DATE,\n",
    "        total_page_views INT,\n",
    "        total_products_viewed INT,\n",
    "        avg_session_duration DECIMAL(8,2),\n",
    "        total_bounces INT,\n",
    "        total_sessions INT,\n",
    "        created_date DATETIME2 DEFAULT GETDATE()\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create all tables\n",
    "    tables_to_create = [\n",
    "        (\"customer_features\", customer_features_sql),\n",
    "        (\"rfm_analysis\", rfm_analysis_sql),\n",
    "        (\"cohort_analysis\", cohort_analysis_sql),\n",
    "        (\"customer_clusters\", customer_clusters_sql),\n",
    "        (\"geographic_analysis\", geographic_analysis_sql),\n",
    "        (\"product_performance\", product_performance_sql),\n",
    "        (\"market_basket_analysis\", market_basket_analysis_sql),\n",
    "        (\"category_performance\", category_performance_sql),\n",
    "        (\"sales_timeseries\", sales_timeseries_sql),\n",
    "        (\"channel_analysis\", channel_analysis_sql),\n",
    "        (\"conversion_funnel\", conversion_funnel_sql),\n",
    "        (\"customer_acquisition\", customer_acquisition_sql),\n",
    "        (\"session_metrics\", session_metrics_sql),\n",
    "        (\"user_journey_flow\", user_journey_flow_sql),\n",
    "        (\"user_behavior_segments\", user_behavior_segments_sql),\n",
    "        (\"clv_features\", clv_features_sql),\n",
    "        (\"churn_features\", churn_features_sql),\n",
    "        (\"executive_summary\", executive_summary_sql),\n",
    "        (\"top_customers\", top_customers_sql),\n",
    "        (\"top_products\", top_products_sql),\n",
    "        (\"dim_customer\", dim_customer_sql),\n",
    "        (\"dim_product\", dim_product_sql),\n",
    "        (\"dim_date\", dim_date_sql),\n",
    "        (\"fact_sales\", fact_sales_sql),\n",
    "        (\"fact_customer_behavior\", fact_customer_behavior_sql)\n",
    "    ]\n",
    "    \n",
    "    for table_name, create_sql in tables_to_create:\n",
    "        create_table_if_not_exists(table_name, create_sql)\n",
    "    \n",
    "    print(f\"✅ Created/verified {len(tables_to_create)} analytics tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e23da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analytics tables...\n",
      "✅ Table analytics.customer_features already exists\n",
      "✅ Table analytics.rfm_analysis already exists\n",
      "✅ Table analytics.cohort_analysis already exists\n",
      "✅ Table analytics.customer_clusters already exists\n",
      "✅ Table analytics.geographic_analysis already exists\n",
      "✅ Table analytics.product_performance already exists\n",
      "✅ Table analytics.market_basket_analysis already exists\n",
      "✅ Table analytics.category_performance already exists\n",
      "✅ Table analytics.sales_timeseries already exists\n",
      "✅ Table analytics.channel_analysis already exists\n",
      "✅ Table analytics.conversion_funnel already exists\n",
      "✅ Table analytics.customer_acquisition already exists\n",
      "✅ Table analytics.session_metrics already exists\n",
      "✅ Table analytics.user_journey_flow already exists\n",
      "✅ Table analytics.user_behavior_segments already exists\n",
      "✅ Table analytics.clv_features already exists\n",
      "✅ Table analytics.churn_features already exists\n",
      "✅ Table analytics.executive_summary already exists\n",
      "✅ Table analytics.top_customers already exists\n",
      "✅ Table analytics.top_products already exists\n",
      "✅ Table analytics.dim_customer already exists\n",
      "✅ Table analytics.dim_product already exists\n",
      "✅ Table analytics.dim_date already exists\n",
      "✅ Table analytics.fact_sales already exists\n",
      "✅ Table analytics.fact_customer_behavior already exists\n",
      "✅ Created/verified 25 analytics tables\n"
     ]
    }
   ],
   "source": [
    "create_all_analytics_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707bf7c",
   "metadata": {},
   "source": [
    "# SECTION 3: DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a126ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_source_pyodbc(conn_str):\n",
    "    \"\"\"Extract tables from SQL Server using pyodbc\"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"EXTRACTING DATA FROM SOURCE DATABASE (pyodbc)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    extraction_config = {\n",
    "        \"customers\": {\"partition_column\": \"customer_id\"},\n",
    "        \"products\": {\"partition_column\": \"product_id\"},\n",
    "        \"inventory\": {\"partition_column\": None},\n",
    "        \"orders\": {\"partition_column\": \"order_id\"},\n",
    "        \"order_items\": {\"partition_column\": \"order_id\"},\n",
    "        \"clickstream\": {\"partition_column\": \"click_id\"}\n",
    "    }\n",
    "    \n",
    "    raw_tables = {}\n",
    "    \n",
    "    # Connect once using the passed connection string\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        for table_name, config in extraction_config.items():\n",
    "            try:\n",
    "                sql = f\"SELECT * FROM {table_name}\"\n",
    "                \n",
    "                # Optional: you could implement batching/partitioning here\n",
    "                df = pd.read_sql(sql, conn)\n",
    "                \n",
    "                print(f\"✅ Extracted {table_name}: {len(df):,} records\")\n",
    "                raw_tables[table_name] = df\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error extracting {table_name}: {e}\")\n",
    "    \n",
    "    return raw_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909fd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTING DATA FROM SOURCE DATABASE (pyodbc)\n",
      "================================================================================\n",
      "✅ Extracted customers: 1,000 records\n",
      "✅ Extracted products: 500 records\n",
      "✅ Extracted inventory: 500 records\n",
      "✅ Extracted orders: 5,000 records\n",
      "✅ Extracted order_items: 8,716 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_186429/2175148971.py:26: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted clickstream: 47,851 records\n"
     ]
    }
   ],
   "source": [
    "source_conn_str = build_odbc_url(ODBC_CONFIG[\"source_db\"])\n",
    "raw_tables = extract_data_from_source_pyodbc(source_conn_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d9de3",
   "metadata": {},
   "source": [
    "# SECTION 4: CUSTOMER ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_customer_analytics():\n",
    "    \"\"\"Comprehensive customer analytics with direct ODBC database storage\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED CUSTOMER ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    customers = raw_tables[\"customers\"]\n",
    "    orders = raw_tables[\"orders\"]\n",
    "    order_items = raw_tables[\"order_items\"]\n",
    "    \n",
    "    # RFM Analysis\n",
    "    def calculate_rfm_analysis():\n",
    "        \"\"\"Calculate RFM scores with advanced segmentation\"\"\"\n",
    "        \n",
    "        print(\"Calculating RFM Analysis...\")\n",
    "        \n",
    "        analysis_date = orders.select(max(\"order_date\")).collect()[0][0]\n",
    "        \n",
    "        # Calculate RFM base metrics\n",
    "        rfm_base = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(\n",
    "                max(\"order_date\").alias(\"last_order_date\"),\n",
    "                count(\"order_id\").alias(\"frequency\"),\n",
    "                sum(\"total_amount\").alias(\"monetary_value\"),\n",
    "                min(\"order_date\").alias(\"first_order_date\"),\n",
    "                avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "            ) \\\n",
    "            .withColumn(\"recency\", datediff(lit(analysis_date), col(\"last_order_date\"))) \\\n",
    "            .withColumn(\"customer_lifespan\", datediff(col(\"last_order_date\"), col(\"first_order_date\")))\n",
    "        \n",
    "        # Calculate percentiles for scoring\n",
    "        percentiles_query = rfm_base.select(\n",
    "            expr(\"percentile_approx(recency, array(0.2, 0.4, 0.6, 0.8))\").alias(\"r_percentiles\"),\n",
    "            expr(\"percentile_approx(frequency, array(0.2, 0.4, 0.6, 0.8))\").alias(\"f_percentiles\"),\n",
    "            expr(\"percentile_approx(monetary_value, array(0.2, 0.4, 0.6, 0.8))\").alias(\"m_percentiles\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        r_p = percentiles_query[\"r_percentiles\"]\n",
    "        f_p = percentiles_query[\"f_percentiles\"]\n",
    "        m_p = percentiles_query[\"m_percentiles\"]\n",
    "        \n",
    "        # Apply RFM scoring\n",
    "        rfm_scored = rfm_base.select(\n",
    "            \"*\",\n",
    "            when(col(\"recency\") <= r_p[0], 5)\n",
    "            .when(col(\"recency\") <= r_p[1], 4)\n",
    "            .when(col(\"recency\") <= r_p[2], 3)\n",
    "            .when(col(\"recency\") <= r_p[3], 2)\n",
    "            .otherwise(1).alias(\"r_score\"),\n",
    "            \n",
    "            when(col(\"frequency\") >= f_p[3], 5)\n",
    "            .when(col(\"frequency\") >= f_p[2], 4)\n",
    "            .when(col(\"frequency\") >= f_p[1], 3)\n",
    "            .when(col(\"frequency\") >= f_p[0], 2)\n",
    "            .otherwise(1).alias(\"f_score\"),\n",
    "            \n",
    "            when(col(\"monetary_value\") >= m_p[3], 5)\n",
    "            .when(col(\"monetary_value\") >= m_p[2], 4)\n",
    "            .when(col(\"monetary_value\") >= m_p[1], 3)\n",
    "            .when(col(\"monetary_value\") >= m_p[0], 2)\n",
    "            .otherwise(1).alias(\"m_score\")\n",
    "        )\n",
    "        \n",
    "        # Create RFM segments\n",
    "        rfm_segments = rfm_scored.withColumn(\n",
    "            \"rfm_segment\",\n",
    "            when((col(\"r_score\") >= 4) & (col(\"f_score\") >= 4) & (col(\"m_score\") >= 4), \"Champions\")\n",
    "            .when((col(\"r_score\") >= 3) & (col(\"f_score\") >= 3) & (col(\"m_score\") >= 3), \"Loyal Customers\")\n",
    "            .when((col(\"r_score\") >= 4) & (col(\"f_score\") <= 2), \"New Customers\")\n",
    "            .when((col(\"r_score\") >= 3) & (col(\"f_score\") <= 2) & (col(\"m_score\") >= 3), \"Potential Loyalists\")\n",
    "            .when((col(\"r_score\") <= 2) & (col(\"f_score\") >= 3) & (col(\"m_score\") >= 3), \"At Risk\")\n",
    "            .when((col(\"r_score\") <= 2) & (col(\"f_score\") <= 2) & (col(\"m_score\") >= 3), \"Cannot Lose Them\")\n",
    "            .when((col(\"r_score\") <= 2) & (col(\"f_score\") <= 2) & (col(\"m_score\") <= 2), \"Lost Customers\")\n",
    "            .otherwise(\"Others\")\n",
    "        ).withColumn(\"rfm_score\", col(\"r_score\") * 100 + col(\"f_score\") * 10 + col(\"m_score\"))\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(rfm_segments, \"rfm_analysis\")\n",
    "        return rfm_segments\n",
    "\n",
    "        # Cohort Analysis\n",
    "    def cohort_analysis():\n",
    "        \"\"\"Advanced cohort analysis with retention metrics\"\"\"\n",
    "        \n",
    "        print(\"Performing Cohort Analysis...\")\n",
    "        \n",
    "        # Get first purchase date for each customer\n",
    "        customer_cohorts = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(min(\"order_date\").alias(\"first_purchase_date\")) \\\n",
    "            .withColumn(\"cohort_month\", date_trunc(\"month\", col(\"first_purchase_date\")))\n",
    "        \n",
    "        # Calculate cohort metrics\n",
    "        cohort_orders = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .join(customer_cohorts, \"customer_id\") \\\n",
    "            .withColumn(\"order_month\", date_trunc(\"month\", col(\"order_date\"))) \\\n",
    "            .withColumn(\n",
    "                \"months_since_first_purchase\",\n",
    "                months_between(col(\"order_month\"), col(\"cohort_month\"))\n",
    "            )\n",
    "        \n",
    "        cohort_table = cohort_orders.groupBy(\"cohort_month\", \"months_since_first_purchase\") \\\n",
    "            .agg(\n",
    "                countDistinct(\"customer_id\").alias(\"customers\"),\n",
    "                count(\"order_id\").alias(\"orders\"),\n",
    "                sum(\"total_amount\").alias(\"revenue\")\n",
    "            )\n",
    "        \n",
    "        # Calculate retention rates\n",
    "        cohort_sizes = cohort_table.filter(col(\"months_since_first_purchase\") == 0) \\\n",
    "            .select(\"cohort_month\", col(\"customers\").alias(\"cohort_size\"))\n",
    "        \n",
    "        cohort_retention = cohort_table.join(cohort_sizes, \"cohort_month\") \\\n",
    "            .withColumn(\"retention_rate\", col(\"customers\") / col(\"cohort_size\")) \\\n",
    "            .withColumn(\"revenue_per_customer\", col(\"revenue\") / col(\"customers\"))\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(cohort_retention, \"cohort_analysis\")\n",
    "        return cohort_retention\n",
    "\n",
    "    def create_customer_features():\n",
    "        \"\"\"Create comprehensive customer features\"\"\"\n",
    "        \n",
    "        print(\"Creating Customer Features...\")\n",
    "        \n",
    "        # Customer transaction patterns\n",
    "        customer_patterns = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(\n",
    "                count(\"order_id\").alias(\"total_orders\"),\n",
    "                sum(\"total_amount\").alias(\"total_spent\"),\n",
    "                avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "                min(\"order_date\").alias(\"first_order_date\"),\n",
    "                max(\"order_date\").alias(\"last_order_date\"),\n",
    "                stddev(\"total_amount\").alias(\"order_value_std\"),\n",
    "                collect_list(\"payment_method\").alias(\"payment_methods_used\")\n",
    "            )\n",
    "        \n",
    "        # Calculate features\n",
    "        current_date = orders.select(max(\"order_date\")).collect()[0][0]\n",
    "        \n",
    "        customer_features = customer_patterns \\\n",
    "            .withColumn(\"customer_age_days\", datediff(col(\"last_order_date\"), col(\"first_order_date\"))) \\\n",
    "            .withColumn(\"days_since_last_order\", datediff(lit(current_date), col(\"last_order_date\"))) \\\n",
    "            .withColumn(\"order_frequency\", col(\"total_orders\") / (col(\"customer_age_days\") + 1)) \\\n",
    "            .withColumn(\"payment_method_diversity\", size(array_distinct(col(\"payment_methods_used\")))) \\\n",
    "            .withColumn(\n",
    "                \"customer_tier\",\n",
    "                when(col(\"total_spent\") >= 1000, \"Premium\")\n",
    "                .when(col(\"total_spent\") >= 500, \"Gold\")\n",
    "                .when(col(\"total_spent\") >= 200, \"Silver\")\n",
    "                .otherwise(\"Bronze\")\n",
    "            )\n",
    "        \n",
    "        # Join with customer demographic data\n",
    "        customer_enriched = customer_features.join(customers, \"customer_id\") \\\n",
    "            .withColumn(\"signup_to_first_order_days\", \n",
    "                       datediff(col(\"first_order_date\"), col(\"signup_date\"))) \\\n",
    "            .select(\n",
    "                \"customer_id\", \"first_name\", \"last_name\", \"email\", \"country\", \"state\", \"city\",\n",
    "                \"segment\", \"customer_tier\", \"total_orders\", \"total_spent\", \"avg_order_value\",\n",
    "                \"customer_age_days\", \"days_since_last_order\", \"order_frequency\", \n",
    "                \"payment_method_diversity\", \"signup_to_first_order_days\"\n",
    "            )\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(customer_enriched, \"customer_features\")\n",
    "        return customer_enriched\n",
    "    \n",
    "    # ML Customer Clustering\n",
    "    def ml_customer_clustering(customer_features, rfm_data):\n",
    "        \"\"\"Advanced customer clustering using ML\"\"\"\n",
    "        \n",
    "        print(\"Performing ML Customer Clustering...\")\n",
    "        \n",
    "        # Combine features for clustering\n",
    "        clustering_data = customer_features.join(rfm_data.select(\"customer_id\", \"r_score\", \"f_score\", \"m_score\"), \"customer_id\")\n",
    "        \n",
    "        # Select numerical features\n",
    "        feature_cols = [\"total_orders\", \"total_spent\", \"avg_order_value\", \"customer_age_days\", \n",
    "                       \"days_since_last_order\", \"order_frequency\", \"r_score\", \"f_score\", \"m_score\"]\n",
    "        \n",
    "        clustering_features = clustering_data.select(\"customer_id\", *feature_cols).fillna(0)\n",
    "        \n",
    "        # Prepare features for ML\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        feature_vector = assembler.transform(clustering_features)\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "        scaler_model = scaler.fit(feature_vector)\n",
    "        scaled_data = scaler_model.transform(feature_vector)\n",
    "        \n",
    "        # Apply K-means clustering\n",
    "        kmeans = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", k=6, seed=42)\n",
    "        kmeans_model = kmeans.fit(scaled_data)\n",
    "        clustered_data = kmeans_model.transform(scaled_data)\n",
    "        \n",
    "        # Add cluster names\n",
    "        cluster_names = clustered_data \\\n",
    "            .withColumn(\"cluster_name\",\n",
    "                       when(col(\"cluster\") == 0, \"High_Value_Loyal\")\n",
    "                       .when(col(\"cluster\") == 1, \"New_Customers\")\n",
    "                       .when(col(\"cluster\") == 2, \"At_Risk_Customers\")\n",
    "                       .when(col(\"cluster\") == 3, \"Low_Value_Occasional\")\n",
    "                       .when(col(\"cluster\") == 4, \"Average_Customers\")\n",
    "                       .otherwise(\"Premium_Customers\")) \\\n",
    "            .select(\"customer_id\", \"cluster\", \"cluster_name\")\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(cluster_names, \"customer_clusters\")\n",
    "        return cluster_names\n",
    "    \n",
    "    # Geographic Analysis\n",
    "    def geographic_analysis(customer_features):\n",
    "        \"\"\"Analyze customer distribution by geography\"\"\"\n",
    "        \n",
    "        print(\"Performing Geographic Analysis...\")\n",
    "        \n",
    "        geo_analysis = customer_features.groupBy(\"country\", \"state\", \"city\") \\\n",
    "            .agg(\n",
    "                count(\"customer_id\").alias(\"customer_count\"),\n",
    "                avg(\"total_spent\").alias(\"avg_customer_value\"),\n",
    "                sum(\"total_spent\").alias(\"total_market_value\"),\n",
    "                avg(\"avg_order_value\").alias(\"avg_order_value\"),\n",
    "                avg(\"total_orders\").alias(\"avg_orders_per_customer\")\n",
    "            ) \\\n",
    "            .withColumn(\"market_penetration_score\", \n",
    "                       col(\"customer_count\") * col(\"avg_customer_value\")) \\\n",
    "            .orderBy(col(\"total_market_value\").desc())\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(geo_analysis, \"geographic_analysis\")\n",
    "        return geo_analysis\n",
    "    \n",
    "    # Execute customer analytics\n",
    "    rfm_analysis = calculate_rfm_analysis()\n",
    "    cohort_data = cohort_analysis()\n",
    "    customer_features = create_customer_features()\n",
    "    customer_clusters = ml_customer_clustering(customer_features, rfm_analysis)\n",
    "    geo_analysis = geographic_analysis(customer_features)\n",
    "    \n",
    "    return {\n",
    "        \"rfm_analysis\": rfm_analysis,\n",
    "        \"cohort_data\": cohort_data,\n",
    "        \"customer_features\": customer_features,\n",
    "        \"customer_clusters\": customer_clusters,\n",
    "        \"geo_analysis\": geo_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADVANCED CUSTOMER ANALYTICS\n",
      "================================================================================\n",
      "Calculating RFM Analysis...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_186429/4250352325.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustomer_analytics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvanced_customer_analytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_186429/3483532185.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0msave_to_database_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"geographic_analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeo_analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;31m# Execute customer analytics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mrfm_analysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_rfm_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mcohort_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohort_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mcustomer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_customer_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mcustomer_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_customer_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfm_analysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_186429/3483532185.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Calculate RFM scores with advanced segmentation\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating RFM Analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0manalysis_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Calculate RFM base metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrfm_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Cancelled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/softsuave/DATA-HDD/DataEngineering/Apache_Airflow/airflow_venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6314\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6316\u001b[0m         ):\n\u001b[1;32m   6317\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "customer_analytics = advanced_customer_analytics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cbf75",
   "metadata": {},
   "source": [
    "# SECTION 5: PRODUCT ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_product_analytics():\n",
    "    \"\"\"Comprehensive product analytics with ODBC database storage\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED PRODUCT ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    products = raw_tables[\"products\"]\n",
    "    inventory = raw_tables[\"inventory\"]\n",
    "    orders = raw_tables[\"orders\"]\n",
    "    order_items = raw_tables[\"order_items\"]\n",
    "    \n",
    "    # Product Performance\n",
    "    def calculate_product_performance():\n",
    "        \"\"\"Calculate comprehensive product performance metrics\"\"\"\n",
    "        \n",
    "        print(\"Calculating Product Performance...\")\n",
    "        \n",
    "        # Basic sales metrics\n",
    "        product_sales = order_items.join(orders.filter(col(\"status\") != \"Cancelled\"), \"order_id\") \\\n",
    "            .groupBy(\"product_id\") \\\n",
    "            .agg(\n",
    "                sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "                sum(\"line_total\").alias(\"total_revenue\"),\n",
    "                count(\"order_item_id\").alias(\"total_line_items\"),\n",
    "                countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "                countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "                avg(\"unit_price\").alias(\"avg_selling_price\"),\n",
    "                min(\"unit_price\").alias(\"min_selling_price\"),\n",
    "                max(\"unit_price\").alias(\"max_selling_price\"),\n",
    "                stddev(\"unit_price\").alias(\"price_volatility\")\n",
    "            )\n",
    "        \n",
    "        # Return metrics\n",
    "        return_metrics = order_items.join(orders, \"order_id\") \\\n",
    "            .groupBy(\"product_id\") \\\n",
    "            .agg(\n",
    "                sum(when(col(\"status\") == \"Returned\", col(\"quantity\")).otherwise(0)).alias(\"returned_quantity\"),\n",
    "                sum(when(col(\"status\") == \"Cancelled\", col(\"quantity\")).otherwise(0)).alias(\"cancelled_quantity\"),\n",
    "                count(\"*\").alias(\"total_order_lines\")\n",
    "            ) \\\n",
    "            .withColumn(\"return_rate\", col(\"returned_quantity\") / col(\"total_order_lines\")) \\\n",
    "            .withColumn(\"cancellation_rate\", col(\"cancelled_quantity\") / col(\"total_order_lines\"))\n",
    "        \n",
    "        # Join all data\n",
    "        product_performance = products.join(product_sales, \"product_id\", \"left\") \\\n",
    "            .join(return_metrics.select(\"product_id\", \"return_rate\", \"cancellation_rate\"), \"product_id\", \"left\") \\\n",
    "            .join(inventory, \"product_id\", \"left\") \\\n",
    "            .fillna(0, [\"total_quantity_sold\", \"total_revenue\", \"return_rate\", \"cancellation_rate\"])\n",
    "        \n",
    "        # Calculate advanced metrics\n",
    "        product_performance = product_performance \\\n",
    "            .withColumn(\"revenue_per_unit\", \n",
    "                       when(col(\"total_quantity_sold\") > 0, col(\"total_revenue\") / col(\"total_quantity_sold\")).otherwise(0)) \\\n",
    "            .withColumn(\"inventory_turnover\", \n",
    "                       when(col(\"stock_level\") > 0, col(\"total_quantity_sold\") / col(\"stock_level\")).otherwise(0)) \\\n",
    "            .withColumn(\"stockout_risk\", \n",
    "                       when(col(\"stock_level\") <= col(\"reorder_threshold\"), \"High\")\n",
    "                       .when(col(\"stock_level\") <= col(\"reorder_threshold\") * 2, \"Medium\")\n",
    "                       .otherwise(\"Low\")) \\\n",
    "            .withColumn(\"profit_margin\", \n",
    "                       when(col(\"price\") > 0, (col(\"avg_selling_price\") - col(\"price\")) / col(\"price\")).otherwise(0)) \\\n",
    "            .withColumn(\"lifecycle_stage\", lit(\"Active\"))  # Simplified for this example\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(product_performance, \"product_performance\")\n",
    "        return product_performance\n",
    "    \n",
    "    # Market Basket Analysis\n",
    "    def market_basket_analysis():\n",
    "        \"\"\"Advanced market basket analysis\"\"\"\n",
    "        \n",
    "        print(\"Performing Market Basket Analysis...\")\n",
    "        \n",
    "        # Products that appear together in orders\n",
    "        order_products = order_items.join(orders.filter(col(\"status\") != \"Cancelled\"), \"order_id\") \\\n",
    "            .select(\"order_id\", \"product_id\") \\\n",
    "            .distinct()\n",
    "        \n",
    "        # Calculate co-occurrence\n",
    "        product_pairs = order_products.alias(\"a\") \\\n",
    "            .join(order_products.alias(\"b\"), col(\"a.order_id\") == col(\"b.order_id\")) \\\n",
    "            .filter(col(\"a.product_id\") < col(\"b.product_id\")) \\\n",
    "            .groupBy(col(\"a.product_id\").alias(\"product_a\"), col(\"b.product_id\").alias(\"product_b\")) \\\n",
    "            .agg(count(\"a.order_id\").alias(\"co_occurrence_count\"))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_orders = orders.filter(col(\"status\") != \"Cancelled\").count()\n",
    "        \n",
    "        product_support = order_products.groupBy(\"product_id\") \\\n",
    "            .agg(count(\"order_id\").alias(\"product_count\")) \\\n",
    "            .withColumn(\"product_support\", col(\"product_count\") / total_orders)\n",
    "        \n",
    "        basket_metrics = product_pairs \\\n",
    "            .join(product_support.alias(\"pa\"), col(\"product_a\") == col(\"pa.product_id\")) \\\n",
    "            .join(product_support.alias(\"pb\"), col(\"product_b\") == col(\"pb.product_id\")) \\\n",
    "            .withColumn(\"support\", col(\"co_occurrence_count\") / total_orders) \\\n",
    "            .withColumn(\"confidence_a_to_b\", col(\"co_occurrence_count\") / col(\"pa.product_count\")) \\\n",
    "            .withColumn(\"confidence_b_to_a\", col(\"co_occurrence_count\") / col(\"pb.product_count\")) \\\n",
    "            .withColumn(\"lift\", col(\"support\") / (col(\"pa.product_support\") * col(\"pb.product_support\"))) \\\n",
    "            .filter(col(\"lift\") > 1.0) \\\n",
    "            .select(\"product_a\", \"product_b\", \"co_occurrence_count\", \"support\", \n",
    "                   \"confidence_a_to_b\", \"confidence_b_to_a\", \"lift\") \\\n",
    "            .orderBy(col(\"lift\").desc()) \\\n",
    "            .limit(1000)  # Limit for database storage\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(basket_metrics, \"market_basket_analysis\")\n",
    "        return basket_metrics\n",
    "    \n",
    "    # Category Performance\n",
    "    def category_performance_analysis():\n",
    "        \"\"\"Analyze performance by product categories\"\"\"\n",
    "        \n",
    "        print(\"Analyzing Category Performance...\")\n",
    "        \n",
    "        # Get product performance data\n",
    "        product_perf = read_from_database_spark_jdbc(\"product_performance\")\n",
    "        \n",
    "        category_metrics = product_perf.groupBy(\"category\") \\\n",
    "            .agg(\n",
    "                count(\"product_id\").alias(\"total_products\"),\n",
    "                sum(\"total_quantity_sold\").alias(\"category_quantity_sold\"),\n",
    "                sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "                avg(\"avg_selling_price\").alias(\"avg_category_price\"),\n",
    "                avg(\"return_rate\").alias(\"avg_return_rate\"),\n",
    "                avg(\"inventory_turnover\").alias(\"avg_inventory_turnover\"),\n",
    "                countDistinct(\"brand\").alias(\"brand_diversity\")\n",
    "            )\n",
    "        \n",
    "        # Calculate market share\n",
    "        total_revenue = category_metrics.select(sum(\"category_revenue\")).collect()[0][0]\n",
    "        \n",
    "        category_share = category_metrics \\\n",
    "            .withColumn(\"market_share\", col(\"category_revenue\") / total_revenue) \\\n",
    "            .withColumn(\"revenue_per_product\", col(\"category_revenue\") / col(\"total_products\")) \\\n",
    "            .orderBy(col(\"category_revenue\").desc())\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_spark(category_share, \"category_performance\")\n",
    "        return category_share\n",
    "    \n",
    "    # Execute product analytics\n",
    "    product_performance = calculate_product_performance()\n",
    "    basket_analysis = market_basket_analysis()\n",
    "    category_analysis = category_performance_analysis()\n",
    "    \n",
    "    return {\n",
    "        \"product_performance\": product_performance,\n",
    "        \"basket_analysis\": basket_analysis,\n",
    "        \"category_analysis\": category_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8119127",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_analytics = advanced_product_analytics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Advanced Sales Analytics\n",
    "def advanced_sales_analytics():\n",
    "    \"\"\"Comprehensive sales analytics with ODBC database storage\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED SALES ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    orders = raw_tables[\"orders\"]\n",
    "    customers = raw_tables[\"customers\"]\n",
    "    \n",
    "    # Time Series Sales Analysis\n",
    "    def sales_time_series_analysis():\n",
    "        \"\"\"Advanced time series analysis\"\"\"\n",
    "        \n",
    "        print(\"Performing Sales Time Series Analysis...\")\n",
    "        \n",
    "        # Daily sales metrics\n",
    "        daily_sales = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .withColumn(\"order_date_only\", to_date(col(\"order_date\"))) \\\n",
    "            .groupBy(\"order_date_only\") \\\n",
    "            .agg(\n",
    "                count(\"order_id\").alias(\"daily_orders\"),\n",
    "                sum(\"total_amount\").alias(\"daily_revenue\"),\n",
    "                avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "                countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "                sum(\"shipping\").alias(\"total_shipping\"),\n",
    "                sum(\"tax\").alias(\"total_tax\"),\n",
    "                sum(\"discount\").alias(\"total_discounts\")\n",
    "            )\n",
    "        \n",
    "        # Add time features\n",
    "        daily_sales_enriched = daily_sales \\\n",
    "            .withColumn(\"day_of_week\", dayofweek(\"order_date_only\")) \\\n",
    "            .withColumn(\"day_name\", date_format(\"order_date_only\", \"EEEE\")) \\\n",
    "            .withColumn(\"month\", month(\"order_date_only\")) \\\n",
    "            .withColumn(\"quarter\", quarter(\"order_date_only\")) \\\n",
    "            .withColumn(\"year\", year(\"order_date_only\")) \\\n",
    "            .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), True).otherwise(False))\n",
    "        \n",
    "        # Calculate moving averages\n",
    "        window_7d = Window.orderBy(\"order_date_only\").rowsBetween(-6, 0)\n",
    "        window_30d = Window.orderBy(\"order_date_only\").rowsBetween(-29, 0)\n",
    "        window_lag = Window.orderBy(\"order_date_only\")\n",
    "        \n",
    "        sales_with_trends = daily_sales_enriched \\\n",
    "            .withColumn(\"revenue_7d_ma\", avg(\"daily_revenue\").over(window_7d)) \\\n",
    "            .withColumn(\"revenue_30d_ma\", avg(\"daily_revenue\").over(window_30d)) \\\n",
    "            .withColumn(\"prev_day_revenue\", lag(\"daily_revenue\").over(window_lag)) \\\n",
    "            .withColumn(\"revenue_growth\", \n",
    "                       when(col(\"prev_day_revenue\") > 0,\n",
    "                            (col(\"daily_revenue\") - col(\"prev_day_revenue\")) / col(\"prev_day_revenue\"))\n",
    "                       .otherwise(0)) \\\n",
    "            .select(\"order_date_only\", \"daily_orders\", \"daily_revenue\", \"avg_order_value\",\n",
    "                   \"unique_customers\", \"total_shipping\", \"total_tax\", \"total_discounts\",\n",
    "                   \"day_of_week\", \"day_name\", \"month\", \"quarter\", \"year\", \"is_weekend\",\n",
    "                   \"revenue_7d_ma\", \"revenue_30d_ma\", \"revenue_growth\")\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(sales_with_trends, \"sales_timeseries\")\n",
    "        return sales_with_trends\n",
    "    \n",
    "    # Channel Analysis\n",
    "    def channel_performance_analysis():\n",
    "        \"\"\"Analyze sales channel performance\"\"\"\n",
    "        \n",
    "        print(\"Analyzing Channel Performance...\")\n",
    "        \n",
    "        channel_performance = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .groupBy(\"placed_via\") \\\n",
    "            .agg(\n",
    "                count(\"order_id\").alias(\"total_orders\"),\n",
    "                sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "                avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "                countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "                sum(\"discount\").alias(\"total_discounts\")\n",
    "            ) \\\n",
    "            .withColumn(\"revenue_per_customer\", col(\"total_revenue\") / col(\"unique_customers\")) \\\n",
    "            .withColumn(\"discount_rate\", col(\"total_discounts\") / col(\"total_revenue\"))\n",
    "        \n",
    "        # Calculate market share\n",
    "        total_revenue = channel_performance.select(sum(\"total_revenue\")).collect()[0][0]\n",
    "        \n",
    "        channel_share = channel_performance \\\n",
    "            .withColumn(\"market_share\", col(\"total_revenue\") / total_revenue) \\\n",
    "            .orderBy(col(\"total_revenue\").desc())\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(channel_share, \"channel_analysis\")\n",
    "        return channel_share\n",
    "    \n",
    "    # Customer Acquisition Analysis\n",
    "    def customer_acquisition_analysis():\n",
    "        \"\"\"Analyze customer acquisition patterns\"\"\"\n",
    "        \n",
    "        print(\"Analyzing Customer Acquisition...\")\n",
    "        \n",
    "        # First-time customers\n",
    "        customer_order_sequence = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .withColumn(\"order_rank\", \n",
    "                       row_number().over(Window.partitionBy(\"customer_id\").orderBy(\"order_date\")))\n",
    "        \n",
    "        new_customers = customer_order_sequence.filter(col(\"order_rank\") == 1) \\\n",
    "            .withColumn(\"acquisition_month\", date_trunc(\"month\", col(\"order_date\"))) \\\n",
    "            .groupBy(\"acquisition_month\") \\\n",
    "            .agg(\n",
    "                count(\"customer_id\").alias(\"new_customers\"),\n",
    "                sum(\"total_amount\").alias(\"new_customer_revenue\"),\n",
    "                avg(\"total_amount\").alias(\"avg_first_order_value\")\n",
    "            )\n",
    "        \n",
    "        # Conversion funnel\n",
    "        signup_to_purchase = customers \\\n",
    "            .join(customer_order_sequence.filter(col(\"order_rank\") == 1), \"customer_id\", \"left\") \\\n",
    "            .withColumn(\"days_to_first_purchase\", \n",
    "                       datediff(col(\"order_date\"), col(\"signup_date\"))) \\\n",
    "            .withColumn(\"converted\", when(col(\"order_date\").isNotNull(), 1).otherwise(0))\n",
    "        \n",
    "        conversion_metrics = signup_to_purchase \\\n",
    "            .withColumn(\"signup_month\", date_trunc(\"month\", col(\"signup_date\"))) \\\n",
    "            .groupBy(\"signup_month\") \\\n",
    "            .agg(\n",
    "                count(\"customer_id\").alias(\"signups\"),\n",
    "                sum(\"converted\").alias(\"conversions\"),\n",
    "                avg(\"days_to_first_purchase\").alias(\"avg_days_to_convert\")\n",
    "            ) \\\n",
    "            .withColumn(\"conversion_rate\", col(\"conversions\") / col(\"signups\"))\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(new_customers, \"customer_acquisition\")\n",
    "        save_to_database_odbc(conversion_metrics, \"conversion_funnel\")\n",
    "        \n",
    "        return new_customers, conversion_metrics\n",
    "    \n",
    "    # Execute sales analytics\n",
    "    sales_timeseries = sales_time_series_analysis()\n",
    "    channel_analysis = channel_performance_analysis()\n",
    "    new_customers, conversion_funnel = customer_acquisition_analysis()\n",
    "    \n",
    "    return {\n",
    "        \"sales_timeseries\": sales_timeseries,\n",
    "        \"channel_analysis\": channel_analysis,\n",
    "        \"new_customers\": new_customers,\n",
    "        \"conversion_funnel\": conversion_funnel\n",
    "    }\n",
    "\n",
    "# Execute sales analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea903490",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_analytics = advanced_sales_analytics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2231347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 8: Advanced Clickstream Analytics\n",
    "def advanced_clickstream_analytics():\n",
    "    \"\"\"Comprehensive clickstream analytics with ODBC database storage\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED CLICKSTREAM ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    clickstream = raw_tables[\"clickstream\"]\n",
    "    \n",
    "    # Session Analysis\n",
    "    def session_analysis():\n",
    "        \"\"\"Advanced session analysis\"\"\"\n",
    "        \n",
    "        print(\"Performing Session Analysis...\")\n",
    "        \n",
    "        # Session-level metrics\n",
    "        session_metrics = clickstream \\\n",
    "            .groupBy(\"session_id\", \"customer_id\", \"device\") \\\n",
    "            .agg(\n",
    "                count(\"click_id\").alias(\"page_views\"),\n",
    "                countDistinct(\"page\").alias(\"unique_pages\"),\n",
    "                countDistinct(\"product_id\").alias(\"products_viewed\"),\n",
    "                min(\"timestamp\").alias(\"session_start\"),\n",
    "                max(\"timestamp\").alias(\"session_end\"),\n",
    "                first(\"utm_source\").alias(\"traffic_source\"),\n",
    "                first(\"referrer\").alias(\"referrer\")\n",
    "            ) \\\n",
    "            .withColumn(\"session_duration_minutes\", \n",
    "                       (unix_timestamp(\"session_end\") - unix_timestamp(\"session_start\")) / 60) \\\n",
    "            .withColumn(\"bounce\", when(col(\"page_views\") == 1, True).otherwise(False)) \\\n",
    "            .withColumn(\"completed_purchase\", \n",
    "                       when(col(\"page_views\") >= 5, True).otherwise(False))  # Simplified logic\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(session_metrics, \"session_metrics\")\n",
    "        \n",
    "        # User journey flow analysis\n",
    "        journey_patterns = clickstream \\\n",
    "            .withColumn(\"page_order\", \n",
    "                       row_number().over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\"))) \\\n",
    "            .withColumn(\"next_page\", \n",
    "                       lead(\"page\").over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\"))) \\\n",
    "            .filter(col(\"next_page\").isNotNull()) \\\n",
    "            .groupBy(\"page\", \"next_page\") \\\n",
    "            .agg(count(\"*\").alias(\"transition_count\")) \\\n",
    "            .withColumn(\"total_from_page\", \n",
    "                       sum(\"transition_count\").over(Window.partitionBy(\"page\"))) \\\n",
    "            .withColumn(\"transition_probability\", \n",
    "                       col(\"transition_count\") / col(\"total_from_page\")) \\\n",
    "            .orderBy(col(\"transition_count\").desc()) \\\n",
    "            .limit(500)  # Limit for database storage\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(journey_patterns, \"user_journey_flow\")\n",
    "        \n",
    "        return session_metrics, journey_patterns\n",
    "    \n",
    "    # User Behavior Segmentation\n",
    "    def user_behavior_segmentation():\n",
    "        \"\"\"Segment users based on browsing behavior\"\"\"\n",
    "        \n",
    "        print(\"Performing User Behavior Segmentation...\")\n",
    "        \n",
    "        # User-level behavioral metrics\n",
    "        user_behavior = clickstream.filter(col(\"customer_id\").isNotNull()) \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(\n",
    "                count(\"click_id\").alias(\"total_clicks\"),\n",
    "                countDistinct(\"session_id\").alias(\"total_sessions\"),\n",
    "                countDistinct(\"page\").alias(\"unique_pages_visited\"),\n",
    "                countDistinct(\"product_id\").alias(\"unique_products_viewed\"),\n",
    "                countDistinct(\"device\").alias(\"devices_used\"),\n",
    "                collect_set(\"utm_source\").alias(\"traffic_sources\")\n",
    "            ) \\\n",
    "            .withColumn(\"clicks_per_session\", col(\"total_clicks\") / col(\"total_sessions\")) \\\n",
    "            .withColumn(\"product_focus\", col(\"unique_products_viewed\") / col(\"total_clicks\")) \\\n",
    "            .withColumn(\"source_diversity\", size(col(\"traffic_sources\")))\n",
    "        \n",
    "        # Calculate percentiles for segmentation\n",
    "        behavior_stats = user_behavior.select(\n",
    "            expr(\"percentile_approx(total_clicks, array(0.33, 0.67))\").alias(\"clicks_p\"),\n",
    "            expr(\"percentile_approx(clicks_per_session, array(0.33, 0.67))\").alias(\"cps_p\"),\n",
    "            expr(\"percentile_approx(product_focus, array(0.33, 0.67))\").alias(\"pf_p\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Apply behavioral segmentation\n",
    "        user_segments = user_behavior \\\n",
    "            .withColumn(\n",
    "                \"engagement_level\",\n",
    "                when(col(\"total_clicks\") >= behavior_stats[\"clicks_p\"][1], \"High\")\n",
    "                .when(col(\"total_clicks\") >= behavior_stats[\"clicks_p\"][0], \"Medium\")\n",
    "                .otherwise(\"Low\")\n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"session_intensity\",\n",
    "                when(col(\"clicks_per_session\") >= behavior_stats[\"cps_p\"][1], \"Intensive\")\n",
    "                .when(col(\"clicks_per_session\") >= behavior_stats[\"cps_p\"][0], \"Moderate\")\n",
    "                .otherwise(\"Light\")\n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"shopping_focus\",\n",
    "                when(col(\"product_focus\") >= behavior_stats[\"pf_p\"][1], \"Product Focused\")\n",
    "                .when(col(\"product_focus\") >= behavior_stats[\"pf_p\"][0], \"Mixed Browsing\")\n",
    "                .otherwise(\"General Browsing\")\n",
    "            ) \\\n",
    "            .select(\"customer_id\", \"total_clicks\", \"total_sessions\", \"unique_pages_visited\",\n",
    "                   \"unique_products_viewed\", \"devices_used\", \"clicks_per_session\", \"product_focus\",\n",
    "                   \"source_diversity\", \"engagement_level\", \"session_intensity\", \"shopping_focus\")\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(user_segments, \"user_behavior_segments\")\n",
    "        return user_segments\n",
    "    \n",
    "    # Execute clickstream analytics\n",
    "    session_metrics, journey_patterns = session_analysis()\n",
    "    behavior_segments = user_behavior_segmentation()\n",
    "    \n",
    "    return {\n",
    "        \"session_metrics\": session_metrics,\n",
    "        \"journey_patterns\": journey_patterns,\n",
    "        \"behavior_segments\": behavior_segments\n",
    "    }\n",
    "\n",
    "# Execute clickstream analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_analytics = advanced_clickstream_analytics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_features():\n",
    "    \"\"\"Create ML features and store in ODBC database\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED ML FEATURE ENGINEERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data from database for ML features\n",
    "    customer_features = read_from_database_odbc(\"customer_features\")\n",
    "    rfm_analysis = read_from_database_odbc(\"rfm_analysis\")\n",
    "    \n",
    "    # CLV Features\n",
    "    def create_clv_features():\n",
    "        \"\"\"Create CLV prediction features\"\"\"\n",
    "        \n",
    "        print(\"Creating CLV Features...\")\n",
    "        \n",
    "        clv_features = customer_features.join(rfm_analysis.select(\"customer_id\", \"r_score\", \"f_score\", \"m_score\"), \"customer_id\") \\\n",
    "            .withColumn(\"orders_per_day\", col(\"total_orders\") / col(\"customer_age_days\")) \\\n",
    "            .withColumn(\"revenue_per_day\", col(\"total_spent\") / col(\"customer_age_days\")) \\\n",
    "            .withColumn(\"high_value_customer\", when(col(\"total_spent\") > 1000, True).otherwise(False)) \\\n",
    "            .select(\"customer_id\", \"total_orders\", \"total_spent\", \"avg_order_value\", \n",
    "                   \"customer_age_days\", \"days_since_last_order\", \"order_frequency\",\n",
    "                   \"r_score\", \"f_score\", \"m_score\", \"high_value_customer\", \n",
    "                   \"orders_per_day\", \"revenue_per_day\")\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(clv_features, \"clv_features\")\n",
    "        return clv_features\n",
    "    \n",
    "    # Churn Features\n",
    "    def create_churn_features():\n",
    "        \"\"\"Create churn prediction features\"\"\"\n",
    "        \n",
    "        print(\"Creating Churn Features...\")\n",
    "        \n",
    "        orders = raw_tables[\"orders\"]\n",
    "        \n",
    "        # Calculate recent activity\n",
    "        latest_orders = orders.filter(col(\"status\") != \"Cancelled\") \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(\n",
    "                max(\"order_date\").alias(\"last_order_date\"),\n",
    "                count(\"order_id\").alias(\"recent_orders\"),\n",
    "                avg(\"total_amount\").alias(\"avg_recent_order_value\")\n",
    "            )\n",
    "        \n",
    "        current_date = orders.select(max(\"order_date\")).collect()[0][0]\n",
    "        \n",
    "        churn_features = latest_orders \\\n",
    "            .withColumn(\"days_since_last_order\", datediff(lit(current_date), col(\"last_order_date\"))) \\\n",
    "            .withColumn(\"churn_risk\", when(col(\"days_since_last_order\") > 90, True).otherwise(False)) \\\n",
    "            .join(customer_features.select(\"customer_id\", \"total_orders\", \"total_spent\", \"avg_order_value\"), \"customer_id\") \\\n",
    "            .join(rfm_analysis.select(\"customer_id\", \"r_score\", \"f_score\", \"m_score\"), \"customer_id\") \\\n",
    "            .withColumn(\"churn_risk_score\", \n",
    "                       (col(\"days_since_last_order\") / 365.0 * 0.4) + \n",
    "                       ((5 - col(\"r_score\")) / 5.0 * 0.3) + \n",
    "                       ((5 - col(\"f_score\")) / 5.0 * 0.3))\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(churn_features, \"churn_features\")\n",
    "        return churn_features\n",
    "    \n",
    "    # Execute ML feature creation\n",
    "    clv_features = create_clv_features()\n",
    "    churn_features = create_churn_features()\n",
    "    \n",
    "    return {\n",
    "        \"clv_features\": clv_features,\n",
    "        \"churn_features\": churn_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_features = create_ml_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15856f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_warehouse():\n",
    "    \"\"\"Create optimized data warehouse with ODBC database storage\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING DATA WAREHOUSE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load analytics data from database using ODBC\n",
    "    customer_features = read_from_database_odbc(\"customer_features\")\n",
    "    customer_clusters = read_from_database_odbc(\"customer_clusters\")\n",
    "    rfm_analysis = read_from_database_odbc(\"rfm_analysis\")\n",
    "    product_performance = read_from_database_odbc(\"product_performance\")\n",
    "    sales_timeseries = read_from_database_odbc(\"sales_timeseries\")\n",
    "    session_metrics = read_from_database_odbc(\"session_metrics\")\n",
    "    \n",
    "    # Customer Dimension\n",
    "    def create_dim_customer():\n",
    "        \"\"\"Create customer dimension table\"\"\"\n",
    "        \n",
    "        print(\"Creating Customer Dimension...\")\n",
    "        \n",
    "        dim_customer = customer_features \\\n",
    "            .join(customer_clusters, \"customer_id\") \\\n",
    "            .join(rfm_analysis.select(\"customer_id\", \"rfm_segment\"), \"customer_id\") \\\n",
    "            .select(\n",
    "                \"customer_id\", \"first_name\", \"last_name\", \"email\", \"country\", \"state\", \"city\",\n",
    "                \"segment\", \"customer_tier\", \"rfm_segment\", \"cluster\", \"cluster_name\", \n",
    "                \"total_orders\", \"total_spent\"\n",
    "            ) \\\n",
    "            .withColumn(\"effective_date\", current_date()) \\\n",
    "            .withColumn(\"is_current\", lit(True))\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(dim_customer, \"dim_customer\")\n",
    "        return dim_customer\n",
    "    \n",
    "    # Product Dimension\n",
    "    def create_dim_product():\n",
    "        \"\"\"Create product dimension table\"\"\"\n",
    "        \n",
    "        print(\"Creating Product Dimension...\")\n",
    "        \n",
    "        dim_product = product_performance.select(\n",
    "            \"product_id\", \"sku\", \"product_name\", \"category\", \"brand\", \"price\",\n",
    "            \"lifecycle_stage\", \"stockout_risk\", \"profit_margin\", \"total_quantity_sold\"\n",
    "        )\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(dim_product, \"dim_product\")\n",
    "        return dim_product\n",
    "    \n",
    "    # Date Dimension\n",
    "    def create_dim_date():\n",
    "        \"\"\"Create date dimension table\"\"\"\n",
    "        \n",
    "        print(\"Creating Date Dimension...\")\n",
    "        \n",
    "        date_range = sales_timeseries.select(\"order_date\").distinct() \\\n",
    "            .withColumnRenamed(\"order_date\", \"date\")\n",
    "        \n",
    "        dim_date = date_range \\\n",
    "            .withColumn(\"year\", year(\"date\")) \\\n",
    "            .withColumn(\"month\", month(\"date\")) \\\n",
    "            .withColumn(\"day\", dayofmonth(\"date\")) \\\n",
    "            .withColumn(\"quarter\", quarter(\"date\")) \\\n",
    "            .withColumn(\"day_of_week\", dayofweek(\"date\")) \\\n",
    "            .withColumn(\"day_name\", date_format(\"date\", \"EEEE\")) \\\n",
    "            .withColumn(\"month_name\", date_format(\"date\", \"MMMM\")) \\\n",
    "            .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), True).otherwise(False)) \\\n",
    "            .withColumn(\"is_month_end\", when(col(\"day\") >= 28, True).otherwise(False)) \\\n",
    "            .withColumn(\"fiscal_year\", when(col(\"month\") >= 4, col(\"year\")).otherwise(col(\"year\") - 1))\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(dim_date, \"dim_date\")\n",
    "        return dim_date\n",
    "    \n",
    "    # Sales Fact\n",
    "    def create_fact_sales():\n",
    "        \"\"\"Create sales fact table\"\"\"\n",
    "        \n",
    "        print(\"Creating Sales Fact...\")\n",
    "        \n",
    "        fact_sales = sales_timeseries.select(\n",
    "            col(\"order_date\").alias(\"date\"), \"daily_orders\", \"daily_revenue\", \"avg_order_value\",\n",
    "            \"unique_customers\", \"total_shipping\", \"total_tax\", \"total_discounts\",\n",
    "            \"revenue_7d_ma\", \"revenue_30d_ma\", \"revenue_growth\"\n",
    "        )\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(fact_sales, \"fact_sales\")\n",
    "        return fact_sales\n",
    "    \n",
    "    # Customer Behavior Fact\n",
    "    def create_fact_customer_behavior():\n",
    "        \"\"\"Create customer behavior fact table\"\"\"\n",
    "        \n",
    "        print(\"Creating Customer Behavior Fact...\")\n",
    "        \n",
    "        fact_customer_behavior = session_metrics \\\n",
    "            .filter(col(\"customer_id\").isNotNull()) \\\n",
    "            .withColumn(\"date\", to_date(\"session_start\")) \\\n",
    "            .groupBy(\"customer_id\", \"date\") \\\n",
    "            .agg(\n",
    "                sum(\"page_views\").alias(\"total_page_views\"),\n",
    "                sum(\"products_viewed\").alias(\"total_products_viewed\"),\n",
    "                avg(\"session_duration_minutes\").alias(\"avg_session_duration\"),\n",
    "                sum(when(col(\"bounce\") == True, 1).otherwise(0)).alias(\"total_bounces\"),\n",
    "                countDistinct(\"session_id\").alias(\"total_sessions\")\n",
    "            )\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(fact_customer_behavior, \"fact_customer_behavior\")\n",
    "        return fact_customer_behavior\n",
    "    \n",
    "    # Execute warehouse creation\n",
    "    dim_customer = create_dim_customer()\n",
    "    dim_product = create_dim_product()\n",
    "    dim_date = create_dim_date()\n",
    "    fact_sales = create_fact_sales()\n",
    "    fact_customer_behavior = create_fact_customer_behavior()\n",
    "    \n",
    "    return {\n",
    "        \"dim_customer\": dim_customer,\n",
    "        \"dim_product\": dim_product,\n",
    "        \"dim_date\": dim_date,\n",
    "        \"fact_sales\": fact_sales,\n",
    "        \"fact_customer_behavior\": fact_customer_behavior\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c12127",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_tables = create_data_warehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afac1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard_and_summary_tables():\n",
    "    \"\"\"Create dashboard and summary tables using ODBC\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING DASHBOARD AND SUMMARY TABLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Executive Summary\n",
    "    def create_executive_summary():\n",
    "        \"\"\"Create executive summary metrics\"\"\"\n",
    "        \n",
    "        print(\"Creating Executive Summary...\")\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        customer_count = read_from_database_odbc(\"customer_features\").count()\n",
    "        \n",
    "        sales_metrics = read_from_database_odbc(\"sales_timeseries\").agg(\n",
    "            sum(\"daily_revenue\").alias(\"total_revenue\"),\n",
    "            avg(\"avg_order_value\").alias(\"avg_order_value\"),\n",
    "            sum(\"daily_orders\").alias(\"total_orders\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Create summary records\n",
    "        summary_data = [\n",
    "            (\"total_customers\", float(customer_count)),\n",
    "            (\"total_revenue\", float(sales_metrics[\"total_revenue\"] or 0)),\n",
    "            (\"avg_order_value\", float(sales_metrics[\"avg_order_value\"] or 0)),\n",
    "            (\"total_orders\", float(sales_metrics[\"total_orders\"] or 0))\n",
    "        ]\n",
    "        \n",
    "        exec_summary_df = spark.createDataFrame(summary_data, [\"metric_name\", \"metric_value\"])\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(exec_summary_df, \"executive_summary\")\n",
    "        return exec_summary_df\n",
    "    \n",
    "    # Top Customers\n",
    "    def create_top_customers():\n",
    "        \"\"\"Create top customers table\"\"\"\n",
    "        \n",
    "        print(\"Creating Top Customers...\")\n",
    "        \n",
    "        customer_features = read_from_database_odbc(\"customer_features\")\n",
    "        \n",
    "        top_customers = customer_features \\\n",
    "            .orderBy(col(\"total_spent\").desc()) \\\n",
    "            .limit(100) \\\n",
    "            .withColumn(\"rank_position\", row_number().over(Window.orderBy(col(\"total_spent\").desc()))) \\\n",
    "            .select(\"rank_position\", \"customer_id\", \"first_name\", \"last_name\", \n",
    "                   \"total_spent\", \"total_orders\", \"customer_tier\")\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(top_customers, \"top_customers\")\n",
    "        return top_customers\n",
    "    \n",
    "    # Top Products\n",
    "    def create_top_products():\n",
    "        \"\"\"Create top products table\"\"\"\n",
    "        \n",
    "        print(\"Creating Top Products...\")\n",
    "        \n",
    "        product_performance = read_from_database_odbc(\"product_performance\")\n",
    "        \n",
    "        top_products = product_performance \\\n",
    "            .orderBy(col(\"total_revenue\").desc()) \\\n",
    "            .limit(100) \\\n",
    "            .withColumn(\"rank_position\", row_number().over(Window.orderBy(col(\"total_revenue\").desc()))) \\\n",
    "            .select(\"rank_position\", \"product_id\", \"product_name\", \"category\", \n",
    "                   \"total_revenue\", \"total_quantity_sold\")\n",
    "        \n",
    "        # Save to database using ODBC\n",
    "        save_to_database_odbc(top_products, \"top_products\")\n",
    "        return top_products\n",
    "    \n",
    "    # Execute dashboard creation\n",
    "    exec_summary = create_executive_summary()\n",
    "    top_customers = create_top_customers()\n",
    "    top_products = create_top_products()\n",
    "    \n",
    "    return {\n",
    "        \"executive_summary\": exec_summary,\n",
    "        \"top_customers\": top_customers,\n",
    "        \"top_products\": top_products\n",
    "    }\n",
    "\n",
    "# Execute dashboard creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data = create_dashboard_and_summary_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_validation_and_monitoring():\n",
    "    \"\"\"Perform final validation and generate monitoring report using ODBC\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL VALIDATION AND MONITORING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count records in all analytics tables\n",
    "    tables_to_check = [\n",
    "        \"customer_features\", \"rfm_analysis\", \"cohort_analysis\", \"customer_clusters\",\n",
    "        \"geographic_analysis\", \"product_performance\", \"market_basket_analysis\", \n",
    "        \"category_performance\", \"sales_timeseries\", \"channel_analysis\",\n",
    "        \"conversion_funnel\", \"customer_acquisition\", \"session_metrics\",\n",
    "        \"user_journey_flow\", \"user_behavior_segments\", \"clv_features\",\n",
    "        \"churn_features\", \"executive_summary\", \"top_customers\", \"top_products\",\n",
    "        \"dim_customer\", \"dim_product\", \"dim_date\", \"fact_sales\", \"fact_customer_behavior\"\n",
    "    ]\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    for table_name in tables_to_check:\n",
    "        try:\n",
    "            df = read_from_database_odbc(table_name)\n",
    "            if df is not None:\n",
    "                count = df.count()\n",
    "                validation_results[table_name] = {\n",
    "                    \"status\": \"SUCCESS\",\n",
    "                    \"record_count\": count\n",
    "                }\n",
    "                print(f\"✅ {table_name}: {count:,} records\")\n",
    "            else:\n",
    "                validation_results[table_name] = {\n",
    "                    \"status\": \"ERROR\",\n",
    "                    \"error\": \"Unable to read table\"\n",
    "                }\n",
    "                print(f\"❌ {table_name}: Unable to read table\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            validation_results[table_name] = {\n",
    "                \"status\": \"ERROR\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"❌ {table_name}: Error - {e}\")\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    total_tables = len(tables_to_check)\n",
    "    successful_tables = len([v for v in validation_results.values() if v[\"status\"] == \"SUCCESS\"])\n",
    "    total_records = sum([v[\"record_count\"] for v in validation_results.values() if v[\"status\"] == \"SUCCESS\"])\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"VALIDATION SUMMARY\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Total Tables: {total_tables}\")\n",
    "    print(f\"Successful Tables: {successful_tables}\")\n",
    "    print(f\"Success Rate: {(successful_tables/total_tables)*100:.1f}%\")\n",
    "    print(f\"Total Records Processed: {total_records:,}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Execute final validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = final_validation_and_monitoring()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_summary_and_cleanup():\n",
    "    \"\"\"Provide final summary and cleanup resources\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ETL PIPELINE EXECUTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Success metrics\n",
    "    successful_tables = len([v for v in validation_results.values() if v[\"status\"] == \"SUCCESS\"])\n",
    "    total_records = sum([v[\"record_count\"] for v in validation_results.values() if v[\"status\"] == \"SUCCESS\"])\n",
    "    \n",
    "    print(f\"✅ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"📊 Analytics Tables Created: {successful_tables}\")\n",
    "    print(f\"📈 Total Records Processed: {total_records:,}\")\n",
    "    print(f\"🗄️ Database: {ODBC_CONFIG['target_db']['database']}\")\n",
    "    print(f\"📋 Schema: analytics\")\n",
    "    print(f\"🔌 Connection: ODBC ({ODBC_CONFIG['target_db']['driver']})\")\n",
    "    \n",
    "    print(f\"\\n🔧 FEATURES IMPLEMENTED:\")\n",
    "    print(f\"  • Advanced Customer Analytics (RFM, Cohort, Clustering)\")\n",
    "    print(f\"  • Product Performance & Market Basket Analysis\")\n",
    "    print(f\"  • Sales Time Series & Channel Analysis\")\n",
    "    print(f\"  • Clickstream & User Behavior Analytics\")\n",
    "    print(f\"  • Machine Learning Feature Engineering\")\n",
    "    print(f\"  • Data Warehouse (Star Schema)\")\n",
    "    print(f\"  • Executive Dashboard Tables\")\n",
    "    print(f\"  • Real-time Performance Monitoring\")\n",
    "    \n",
    "    print(f\"\\n📋 ANALYTICS TABLES AVAILABLE:\")\n",
    "    for table_name, result in validation_results.items():\n",
    "        if result[\"status\"] == \"SUCCESS\":\n",
    "            print(f\"  • analytics.{table_name} ({result['record_count']:,} records)\")\n",
    "    \n",
    "    print(f\"\\n🔌 ODBC CONNECTION BENEFITS:\")\n",
    "    print(f\"  • Native database connectivity\")\n",
    "    print(f\"  • Optimized for Windows environments\")\n",
    "    print(f\"  • Better performance for SQL Server\")\n",
    "    print(f\"  • Enterprise security compliance\")\n",
    "    print(f\"  • Direct integration with BI tools\")\n",
    "    \n",
    "    print(f\"\\n🚀 READY FOR:\")\n",
    "    print(f\"  • Power BI / Tableau Dashboards\")\n",
    "    print(f\"  • Excel Analytics & Reporting\")\n",
    "    print(f\"  • SSRS Report Services\")\n",
    "    print(f\"  • Machine Learning Model Training\")\n",
    "    print(f\"  • Real-time Analytics Applications\")\n",
    "    print(f\"  • Executive Reporting\")\n",
    "    print(f\"  • Customer Segmentation\")\n",
    "    print(f\"  • Product Recommendations\")\n",
    "    print(f\"  • Churn Prediction\")\n",
    "    print(f\"  • Revenue Forecasting\")\n",
    "    \n",
    "    # Cleanup cached tables\n",
    "    try:\n",
    "        for table_name in raw_tables.keys():\n",
    "            if hasattr(raw_tables[table_name], 'unpersist'):\n",
    "                raw_tables[table_name].unpersist()\n",
    "        \n",
    "        spark.catalog.clearCache()\n",
    "        print(f\"\\n🧹 Cleanup completed - cache cleared\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup warning: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 E-COMMERCE ANALYTICS PLATFORM READY FOR PRODUCTION!\")\n",
    "    print(\"🔌 FULLY OPTIMIZED FOR ODBC CONNECTIVITY!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216be01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_and_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"✅ Spark session stopped. ODBC ETL pipeline execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
